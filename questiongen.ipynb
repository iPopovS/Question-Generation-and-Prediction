{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14398320,"sourceType":"datasetVersion","datasetId":9195505},{"sourceId":14533393,"sourceType":"datasetVersion","datasetId":9282286}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom typing import List, Dict, Tuple, Optional\nimport json\nimport re\nimport torch\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:02.382234Z","iopub.execute_input":"2026-01-18T04:43:02.382490Z","iopub.status.idle":"2026-01-18T04:43:07.688031Z","shell.execute_reply.started":"2026-01-18T04:43:02.382459Z","shell.execute_reply":"2026-01-18T04:43:07.687244Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet'}\ndf = pd.read_parquet(\"hf://datasets/stanfordnlp/coqa/\" + splits[\"train\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:07.689595Z","iopub.execute_input":"2026-01-18T04:43:07.690023Z","iopub.status.idle":"2026-01-18T04:43:12.519395Z","shell.execute_reply.started":"2026-01-18T04:43:07.689988Z","shell.execute_reply":"2026-01-18T04:43:12.518834Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"rows_numb = []\nfor row in range(len(df['questions'])):\n    if len(df['questions'][row]) == 20:\n        rows_numb.append(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:12.520305Z","iopub.execute_input":"2026-01-18T04:43:12.520541Z","iopub.status.idle":"2026-01-18T04:43:12.553109Z","shell.execute_reply.started":"2026-01-18T04:43:12.520519Z","shell.execute_reply":"2026-01-18T04:43:12.552441Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"rows_100 = rows_numb[:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows_1000 = rows_numb[:1000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:12.553758Z","iopub.execute_input":"2026-01-18T04:43:12.553983Z","iopub.status.idle":"2026-01-18T04:43:12.570715Z","shell.execute_reply.started":"2026-01-18T04:43:12.553954Z","shell.execute_reply":"2026-01-18T04:43:12.570084Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# qq = [\n# \"When was the Vat formally opened?\",\n# \"what is the library for?\",\n# \"for what subjects?\",\n# \"and?\",\n# \"what was started in 2014?\",\n# \"how do scholars divide the library?\",\n# \"how many?\",\n# \"what is the official name of the Vat?\",\n# \"where is it?\",\n# \"how many printed books does it contain?\",\n# \"when were the Secret Archives moved from the rest of the library?\",\n# \"how many items are in this secret collection?\",\n# \"Can anyone use this library?\",\n# \"what must be requested to view?\",\n# \"what must be requested in person or by mail?\",\n# \"of what books?\",\n# \"What is the Vat the library of?\",\n# \"How many books survived the Pre Lateran period?\",\n# \"what is the point of the project started in 2014?\",\n# \"what will this allow?\"\n# ]\n\n# my_questions = qq[:int(len(qq) * 0.6)]\n# my_questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T08:00:50.137081Z","iopub.execute_input":"2026-01-05T08:00:50.137451Z","iopub.status.idle":"2026-01-05T08:00:50.148674Z","shell.execute_reply.started":"2026-01-05T08:00:50.137418Z","shell.execute_reply":"2026-01-05T08:00:50.147524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_json_from_response(response: str) -> Optional[Dict]:\n    \"\"\"Extract and parse JSON from LLM response with error handling.\"\"\"\n    try:\n        # Try direct JSON parsing first\n        return json.loads(response)\n    except json.JSONDecodeError:\n        # Look for JSON-like structure in the response\n        json_match = re.search(r'\\{[^{}]*\\}', response)\n        if json_match:\n            try:\n                return json.loads(json_match.group(0))\n            except json.JSONDecodeError:\n                pass\n    \n    # Fallback: try to find confidence value directly\n    confidence_match = re.search(r'\"confidence\":\\s*([0-9]*\\.?[0-9]+)', response)\n    if confidence_match:\n        return {\"confidence\": float(confidence_match.group(1))}\n    \n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:12.571518Z","iopub.execute_input":"2026-01-18T04:43:12.571791Z","iopub.status.idle":"2026-01-18T04:43:12.585532Z","shell.execute_reply.started":"2026-01-18T04:43:12.571767Z","shell.execute_reply":"2026-01-18T04:43:12.584888Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-4B-Instruct-2507\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:12.586407Z","iopub.execute_input":"2026-01-18T04:43:12.586682Z","iopub.status.idle":"2026-01-18T04:44:36.887519Z","shell.execute_reply.started":"2026-01-18T04:43:12.586655Z","shell.execute_reply":"2026-01-18T04:44:36.886575Z"}},"outputs":[{"name":"stderr","text":"2026-01-18 04:43:20.169333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768711400.398076      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768711400.464153      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768711401.025249      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768711401.025289      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768711401.025291      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768711401.025294      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9130751304f0497f90984fa44ad19349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"413404477b58475997640bf41c08d4b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde1c4548d9a444c8dab831e8f01d22f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09775e0cc52942a1b6e7f69392f16109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9149fd3ca0fb4d8fa045f07a3fe707c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"514000a9551940d5ad7efee4c9577408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8883378f8814b969ff7d44f454258d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bded6bcc9134252a5a8090cdf0c90e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c198411a0b6f41e8838351df8fa4d7ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a75795ac4be4437a3c5fbd03c7f6a34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63d34de10fc44d0a01edbaf9f3f37f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b49c7140e184c108c8f5061d35698da"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# from transformers import pipeline\n\n# pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-7B-Instruct\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def extract_main_topic(pipe, questions: list[str], temperature: float = 0.7) -> str:\n#     \"\"\"Extract the main topic/theme from a set of related questions from the same user context.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a topic extraction expert. Your task is to analyze multiple questions from the same user and identify the single main topic or theme that connects all of them.\n\n# **Instructions:**\n# 1. Analyze all the provided questions together\n# 2. Identify the core subject, domain, or theme that unifies these questions\n# 3. The questions are from the same user context, so they likely revolve around one central topic\n# 4. Output ONLY a JSON object with a single \"topic\" field\n# 5. The main topic should be concise (2-8 words maximum) and capture the essence of all questions\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"topic\": \"possibility of intelligent life existing on other planets\"\n# }\n# \"\"\"\n    \n#     questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n#     user_prompt = f\"\"\"\n# **Questions from the same user context:**\n# {questions_formatted}\n\n# Now analyze these questions and extract the single main topic that connects them all:\n# \"\"\"\n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n\n#     response = pipe(\n#             messages,\n#             temperature=temperature,\n#             do_sample=False,\n#             pad_token_id=pipe.tokenizer.eos_token_id\n#         )\n        \n#     # Extract generated text\n#     if isinstance(response, list) and len(response) > 0:\n#         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#             generated_text = response[0]['generated_text'][-1]['content']\n#         else:\n#             generated_text = str(response[0])\n#     else:\n#         generated_text = str(response)\n    \n#     result = json.loads(generated_text)\n#     if \"topic\" in result:\n#         return str(result[\"topic\"]).strip()\n        \n#     # topic_match = re.search(r'\"topic\":\\s*\"([^\"]+)\"', generated_text)\n#     #     if topic_match:\n#     #         return topic_match.group(1).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T04:32:49.297851Z","iopub.execute_input":"2026-01-04T04:32:49.298484Z","iopub.status.idle":"2026-01-04T04:32:49.305306Z","shell.execute_reply.started":"2026-01-04T04:32:49.298456Z","shell.execute_reply":"2026-01-04T04:32:49.304736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# topic = extract_main_topic(pipe, my_questions)\n# topic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T04:32:54.109389Z","iopub.execute_input":"2026-01-04T04:32:54.109949Z","iopub.status.idle":"2026-01-04T04:32:55.816497Z","shell.execute_reply.started":"2026-01-04T04:32:54.109916Z","shell.execute_reply":"2026-01-04T04:32:55.815084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def extract_topic_aspects(pipe, topic: str, questions: list[str], temperature: float = 0.7) -> list[str]:\n#     \"\"\"Extract specific aspects/details of a known topic that the user is interested in based on their questions.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are an aspect extraction expert. Your task is to analyze questions about a specific topic and identify the key aspects, details, or dimensions of that topic the user is interested in.\n\n# **Instructions:**\n# 1. You are given a main topic and multiple questions about that topic from the same user\n# 2. Analyze the questions to identify what specific aspects, details, or sub-topics of the main topic the user cares about\n# 3. Focus on concrete details mentioned or implied in the questions\n# 4. Output ONLY a JSON object with a single \"aspects\" field containing a list of strings\n# 5. Each aspect should be concise (2-6 words) and represent a distinct detail or dimension of the main topic\n# 6. Include only aspects that are clearly supported by the questions\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"aspects\": [\"scientific reasoning for existence\", \"detection methods and technology\", \"expert predictions and timeline\"]\n# }\n\n# **Examples:**\n# - Topic: \"possibility of intelligent life existing on other planets\" + Questions about evidence, which planet, methods, timeline, and experts → [age of universe, Earth-like planets,  radio telescopes, timeline for contact]\n# - Topic: \"university\" + Questions about founders, buildings, programs → [\"founding history\", \"campus architecture\", \"academic programs\"]\n# \"\"\"\n    \n#     questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n#     user_prompt = f\"\"\"\n# **Main Topic:** \"{topic}\"\n\n# **User Questions about this topic:**\n# {questions_formatted}\n\n# Now analyze these questions and extract the specific aspects or details of \"{topic}\" that the user is interested in:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n    \n#     response = pipe(\n#         messages,\n#         temperature=temperature,\n#         do_sample=False,\n#         pad_token_id=pipe.tokenizer.eos_token_id\n#     )\n    \n#     # Extract generated text\n#     if isinstance(response, list) and len(response) > 0:\n#         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#             generated_text = response[0]['generated_text'][-1]['content']\n#         else:\n#             generated_text = str(response[0])\n#     else:\n#         generated_text = str(response)\n    \n#     # Extract JSON from response\n#     result = json.loads(generated_text)\n#     if \"aspects\" in result and isinstance(result[\"aspects\"], list):\n#         return [str(aspect).strip() for aspect in result[\"aspects\"] if aspect]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:39:04.510215Z","iopub.execute_input":"2026-01-03T11:39:04.510516Z","iopub.status.idle":"2026-01-03T11:39:04.521842Z","shell.execute_reply.started":"2026-01-03T11:39:04.510487Z","shell.execute_reply":"2026-01-03T11:39:04.520815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# aspects = extract_topic_aspects(pipe, topic, my_questions)\n# # aspects","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:39:04.522827Z","iopub.execute_input":"2026-01-03T11:39:04.523121Z","iopub.status.idle":"2026-01-03T11:42:50.235044Z","shell.execute_reply.started":"2026-01-03T11:39:04.523095Z","shell.execute_reply":"2026-01-03T11:42:50.234192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def generate_questions(pipe, topic: str, aspects: list[str], questions: list[str], temperature: float = 0.7) -> list[str]:\n#     \"\"\"Generate possible follow-up questions a user might ask based on topic, aspects, and previously asked questions.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a question generation expert. Your task is to generate new, relevant questions that a user might ask about a specific topic, based on the aspects they're interested in and avoiding questions they've already asked.\n\n# **Instructions:**\n# 1. You are given a main topic, key aspects of that topic, and questions the user has already asked\n# 2. Generate 8 NEW questions that:\n#    - Are relevant to the main topic\n#    - Cover the specified aspects that haven't been fully explored yet\n#    - Are different from the already asked questions (avoid rephrasing the same questions)\n#    - Sound natural and like something a real user would ask\n#    - Focus on details, specifics, or related information the user might want to know next\n# 3. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n# 4. Each question should be clear, concise, and self-contained\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"questions\": [\n#     \"What galaxy do we live in?\",\n#     \"How many stars does it have?\",\n#     \"Does Shostak believe beings from space could make contact with Earth soon?\"\n#   ]\n# }\n\n# **Important Guidelines:**\n# - DO NOT repeat or rephrase the already asked questions\n# - DO cover different aspects or dive deeper into aspects that weren't fully explored\n# - DO make questions specific and actionable\n# - DO maintain a natural, conversational tone\n# - DO focus on what the user might want to know next based on their interests\n# \"\"\"\n    \n#     aspects_formatted = \", \".join([f'\"{aspect}\"' for aspect in aspects])\n#     questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n#     user_prompt = f\"\"\"\n# **Main Topic:** \"{topic}\"\n\n# **Key Aspects of Interest:** {aspects_formatted}\n\n# **Questions Already Asked:**\n# {questions_formatted}\n\n# Now generate new, relevant questions that the user might ask next. These questions should explore the topic and aspects in new ways, avoiding repetition of the already asked questions:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n    \n#     # try:\n#     #     response = pipe(\n#     #         messages,\n#     #         temperature=temperature,\n#     #         do_sample=True,\n#     #         pad_token_id=pipe.tokenizer.eos_token_id\n#     #     )\n        \n#     #     # Extract generated text\n#     #     if isinstance(response, list) and len(response) > 0:\n#     #         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#     #             generated_text = response[0]['generated_text'][-1]['content']\n#     #         else:\n#     #             generated_text = str(response[0])\n#     #     else:\n#     #         generated_text = str(response)\n        \n#     #     # Extract JSON from response\n#     #     result = json.loads(generated_text)\n#     #     if \"questions\" in result and isinstance(result[\"questions\"], list):\n#     #         print('sfsegergerg1111')\n#     #         return [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#     # except json.JSONDecodeError:\n#     #     # Try to find JSON pattern in the response\n#     #     json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n#     #     if json_match:\n#     #         result = json.loads(json_match.group(0))\n#     #         if \"questions\" in result and isinstance(result[\"questions\"], list):\n#     #             print('sfsegergerg2222')\n#     #             return [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n\n#     response = pipe(\n#         messages,\n#         temperature=temperature,\n#         do_sample=True,\n#         pad_token_id=pipe.tokenizer.eos_token_id\n#     )\n    \n#     # Extract generated text\n#     if isinstance(response, list) and len(response) > 0:\n#         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#             generated_text = response[0]['generated_text'][-1]['content']\n#         else:\n#             generated_text = str(response[0])\n#     else:\n#         generated_text = str(response)\n    \n#     # Extract JSON from response\n#     try:\n#         result = json.loads(generated_text)\n#         if \"questions\" in result and isinstance(result[\"questions\"], list):\n#             # Filter out empty questions and limit to 20\n#             questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#             return questions\n#     except json.JSONDecodeError:\n#         # Try to find JSON pattern in the response\n#         json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n#         if json_match:\n#             result = json.loads(json_match.group(0))\n#             if \"questions\" in result and isinstance(result[\"questions\"], list):\n#                 questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#                 return questions\n    \n#     # # Fallback: try to extract questions directly from text\n#     # questions_match = re.search(r'\"questions\":\\s*\\[([^\\]]+)\\]', generated_text)\n#     # if questions_match:\n#     #     questions_str = questions_match.group(1)\n#     #     # Extract individual questions from the list string\n#     #     questions = [q.strip().strip('\"').strip(\"'\") for q in questions_str.split(',') if q.strip()]\n#     #     return questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:42:50.237398Z","iopub.execute_input":"2026-01-03T11:42:50.237733Z","iopub.status.idle":"2026-01-03T11:42:50.251248Z","shell.execute_reply.started":"2026-01-03T11:42:50.237704Z","shell.execute_reply":"2026-01-03T11:42:50.250193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gen_questions = generate_questions(pipe, topic, aspects, my_questions)\n# gen_questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:42:50.252327Z","iopub.execute_input":"2026-01-03T11:42:50.252673Z","iopub.status.idle":"2026-01-03T11:48:00.620714Z","shell.execute_reply.started":"2026-01-03T11:42:50.252647Z","shell.execute_reply":"2026-01-03T11:48:00.619417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gen_questions = generate_questions(pipe, topic, aspects, my_questions)\n# gen_questions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def generate_questions2(pipe, topic: str, aspects: list[str], temperature: float = 0.7, max_new_tokens: int = 200) -> list[str]:\n#     \"\"\"Generate possible follow-up questions a user might ask based on topic, aspects, and previously asked questions.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a question generation expert. Your task is to generate new, relevant questions that a user might ask about a specific topic, based on the aspects they're interested in.\n\n# **Instructions:**\n# 1. You are given a main topic and key aspects of that topic\n# 2. Generate 5-8 NEW questions that:\n#    - Are relevant to the main topic\n#    - Cover the specified aspects that haven't been fully explored yet\n#    - Sound natural and like something a real user would ask\n#    - Focus on details, specifics, or related information the user might want to know next\n# 3. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n# 4. Each question should be clear, concise, and self-contained\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"questions\": [\n#     \"What galaxy do we live in?\",\n#     \"How many stars does it have?\",\n#     \"Does Shostak believe beings from space could make contact with Earth soon?\"\n#   ]\n# }\n\n# **Important Guidelines:**\n# - DO cover different aspects or dive deeper into aspects that weren't fully explored\n# - DO make questions specific and actionable\n# - DO maintain a natural, conversational tone\n# - DO focus on what the user might want to know next based on their interests\n# \"\"\"\n    \n#     aspects_formatted = \", \".join([f'\"{aspect}\"' for aspect in aspects])\n#     # questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n#     user_prompt = f\"\"\"\n# **Main Topic:** \"{topic}\"\n\n# **Key Aspects of Interest:** {aspects_formatted}\n\n# Now generate new, relevant questions that the user might ask next. These questions should explore the topic and aspects in new ways:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n    \n#     try:\n#         response = pipe(\n#             messages,\n#             temperature=temperature,\n#             max_new_tokens=max_new_tokens,\n#             do_sample=True,\n#             pad_token_id=pipe.tokenizer.eos_token_id\n#         )\n        \n#         # Extract generated text\n#         if isinstance(response, list) and len(response) > 0:\n#             if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#                 generated_text = response[0]['generated_text'][-1]['content']\n#             else:\n#                 generated_text = str(response[0])\n#         else:\n#             generated_text = str(response)\n        \n#         # Extract JSON from response\n#         result = json.loads(generated_text)\n#         if \"questions\" in result and isinstance(result[\"questions\"], list):\n#             return [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#     except json.JSONDecodeError:\n#         # Try to find JSON pattern in the response\n#         json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n#         if json_match:\n#             result = json.loads(json_match.group(0))\n#             if \"questions\" in result and isinstance(result[\"questions\"], list):\n#                 return [str(q).strip() for q in result[\"questions\"] if q and q.strip()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gen_questions3 = generate_questions2(pipe, topic, aspects)\n# gen_questions3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# confidence = compare_questions(pipe, 'Has the size or composition of the secret collection changed over the years?', 'How many books survived the Pre Lateran period?')\n# confidence","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_questions(pipe, question1: str, question2: str, \n                     temperature: float = 0.7, max_new_tokens: int = 100) -> float:\n\n    sys_prompt = \"\"\"\nYou are a semantic equivalence evaluator. Your task is to compare two questions and determine the confidence that they are semantically equivalent.\n\n**Instructions:**\n1. Compare the meaning and intent of the two questions\n2. Determine if they're asking for the same information and would yield the same answer\n3. Output ONLY a JSON object with a single \"confidence\" field\n4. The confidence must be a number between 0 and 1\n\n**Explanation of confidence scores:**\n- 0.9-1.0: Definitely equivalent (same meaning, intent, and expected answer)\n- 0.7-0.9: Highly likely equivalent (minor wording differences only)\n- 0.5-0.7: Possibly equivalent (similar intent but different phrasing)\n- 0.3-0.5: Unlikely equivalent (related but different aspects)\n- 0.0-0.3: Definitely different (completely different questions)\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"confidence\": 0.85\n}\n\"\"\"\n\n    prompt = f\"\"\"\n**Questions to compare:**\nQuestion 1: \"{question1}\"\nQuestion 2: \"{question2}\"\n\"\"\"\n\n    messages = [\n    {\"role\": \"system\", \"content\": sys_prompt.strip()},\n    {\"role\": \"user\", \"content\": prompt.strip()}\n]\n\n    response = pipe(\n        messages,\n        temperature=temperature,\n        do_sample=False\n    )\n\n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            # Handle pipeline output format\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            # Handle different output format\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n    \n    # Extract JSON from response\n    result = extract_json_from_response(generated_text)\n    \n    if result and \"confidence\" in result:\n        confidence = float(result[\"confidence\"])\n        # Clamp to 0-1 range\n        return max(0.0, min(1.0, confidence))\n    else:\n        print(f\"Warning: Could not parse confidence from response: {generated_text}\")\n        return 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:36.893784Z","iopub.execute_input":"2026-01-18T04:44:36.894118Z","iopub.status.idle":"2026-01-18T04:44:36.901797Z","shell.execute_reply.started":"2026-01-18T04:44:36.894089Z","shell.execute_reply":"2026-01-18T04:44:36.900342Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# a = compare_questions(pipe, 'What is the Vat the library of?', 'What is the Vat the library of?')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T06:04:52.800764Z","iopub.execute_input":"2026-01-03T06:04:52.801317Z","iopub.status.idle":"2026-01-03T06:04:52.815712Z","shell.execute_reply.started":"2026-01-03T06:04:52.801294Z","shell.execute_reply":"2026-01-03T06:04:52.815098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for row in rows_100:\n#     print(row)\n#     questions = df['questions'].loc[row].tolist()[:int(len(df['questions'].loc[row].tolist()) * 0.6)]\n#     topic = extract_main_topic(pipe, questions)\n#     aspects = extract_topic_aspects(pipe, topic, questions)\n#     gen_questions = generate_questions(pipe, topic, aspects, questions)\n\n#     comparisons = {}\n#     remaining_questions = df['questions'].loc[row].tolist()[int(len(df['questions'].loc[row].tolist()) * 0.6):]\n#     for i, q_a in enumerate(gen_questions):\n#         comparisons[q_a] = {}\n#         for j, q_b in enumerate(remaining_questions):\n#             confidence = compare_questions(pipe, q_a, q_b)\n#             comparisons[q_a][q_b] = confidence\n\n#     results[f'row {row}'] = {\n#     \"topic\": topic,\n#     \"aspects\": aspects,\n#     \"gen_questions\": gen_questions,\n#     \"comparisons\": comparisons\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_df = pd.DataFrame(results)\n# results_df.T","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_df.T.to_csv('/kaggle/working/results_app1.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def generate_questions_from_text(pipe, text: str, temperature: float = 0.7, max_new_tokens: int = 500) -> list[str]:\n#     \"\"\"Generate 10-15 relevant questions based on provided text content.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a question generation expert. Your task is to analyze a given text passage and generate relevant, insightful questions that a reader might ask about the content.\n\n# **Instructions:**\n# 1. Carefully read and understand the provided text\n# 2. Generate 8 diverse questions that:\n#    - Cover key facts, details, and concepts from the text\n#    - Explore implications, context, and deeper meaning\n#    - Ask about specific examples, dates, names, or events mentioned\n#    - Probe relationships between different elements in the text\n#    - Question assumptions or explore alternative perspectives\n#    - Are natural and conversational (not robotic)\n# 3. Ensure questions are:\n#    - Clear, concise, and self-contained\n#    - Based directly on the text content (don't invent facts)\n#    - Varied in type (who, what, when, where, why, how, etc.)\n#    - Appropriate for the text's complexity and tone\n# 4. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n# 5. Include exactly 8 questions - no more, no less\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"questions\": [\n#     \"What galaxy do we live in?\",\n#     \"How many stars does it have?\",\n#     \"Does Shostak believe beings from space could make contact with Earth soon?\"\n#   ]\n# }\n# \"\"\"\n    \n#     user_prompt = f\"\"\"\n# **Text to analyze:**\n# {text}\n\n# Now generate 10-15 thoughtful questions based on this text content:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n    \n#     try:\n#         response = pipe(\n#             messages,\n#             temperature=temperature,\n#             max_new_tokens=max_new_tokens,\n#             do_sample=True,\n#             pad_token_id=pipe.tokenizer.eos_token_id\n#         )\n        \n#         # Extract generated text\n#         if isinstance(response, list) and len(response) > 0:\n#             if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#                 generated_text = response[0]['generated_text'][-1]['content']\n#             else:\n#                 generated_text = str(response[0])\n#         else:\n#             generated_text = str(response)\n        \n#         # Extract JSON from response\n#         try:\n#             result = json.loads(generated_text)\n#             if \"questions\" in result and isinstance(result[\"questions\"], list):\n#                 # Filter out empty questions and limit to 20\n#                 questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#                 return questions\n#         except json.JSONDecodeError:\n#             # Try to find JSON pattern in the response\n#             json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n#             if json_match:\n#                 try:\n#                     result = json.loads(json_match.group(0))\n#                     if \"questions\" in result and isinstance(result[\"questions\"], list):\n#                         questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#                         return questions\n#                 except json.JSONDecodeError:\n#                     pass\n        \n#         # Fallback: try to extract questions directly from text\n#         questions_match = re.search(r'\"questions\":\\s*\\[([^\\]]+)\\]', generated_text)\n#         if questions_match:\n#             questions_str = questions_match.group(1)\n#             # Extract individual questions from the list string\n#             questions = [q.strip().strip('\"').strip(\"'\") for q in questions_str.split(',') if q.strip()]\n#             return questions\n        \n#         print(f\"Warning: Could not parse generated questions from response: {generated_text}\")\n#         return []\n            \n#     except Exception as e:\n#         print(f\"Error generating questions from text: {e}\")\n#         return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T12:10:39.433503Z","iopub.execute_input":"2026-01-03T12:10:39.433755Z","iopub.status.idle":"2026-01-03T12:10:39.444224Z","shell.execute_reply.started":"2026-01-03T12:10:39.433720Z","shell.execute_reply":"2026-01-03T12:10:39.443557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def generate_questions_from_text_with_questions(pipe, text: str, questions: list[str], temperature: float = 0.7, max_new_tokens: int = 500) -> list[str]:\n#     \"\"\"Generate 10-15 relevant questions based on provided text content.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a question generation expert. Your task is to analyze a given text passage and generate relevant, insightful questions that a reader might ask about the content, while avoiding questions that have already been asked.\n\n# **Instructions:**\n# 1. Carefully read and understand the provided text\n# 2. Review the list of already asked questions to avoid duplication\n# 3. Generate 8 diverse questions that:\n#    - Cover key facts, details, and concepts from the text that haven't been explored yet\n#    - Explore implications, context, and deeper meaning not covered in existing questions\n#    - Ask about specific examples, dates, names, or events mentioned in the text\n#    - Probe relationships between different elements in the text\n#    - Question assumptions or explore alternative perspectives\n#    - Are natural and conversational (not robotic)\n# 4. Ensure questions are:\n#    - Clear, concise, and self-contained\n#    - Based directly on the text content (don't invent facts)\n#    - Varied in type (who, what, when, where, why, how, etc.)\n#    - Appropriate for the text's complexity and tone\n#    - COMPLETELY DIFFERENT from the already asked questions (no rephrasing or similar intent)\n# 5. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n# 6. Include exactly 8 questions - no more, no less\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"questions\": [\n#     \"What galaxy do we live in?\",\n#     \"How many stars does it have?\",\n#     \"Does Shostak believe beings from space could make contact with Earth soon?\"\n#   ]\n# }\n# \"\"\"\n\n#     questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n#     user_prompt = f\"\"\"\n# **Text to analyze:**\n# {text}\n\n# **Questions Already Asked:**\n# {questions_formatted}\n\n# Now generate 8 thoughtful questions based on this text content:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n    \n#     response = pipe(\n#         messages,\n#         temperature=temperature,\n#         max_new_tokens=max_new_tokens,\n#         do_sample=True,\n#         pad_token_id=pipe.tokenizer.eos_token_id\n#     )\n    \n#     # Extract generated text\n#     if isinstance(response, list) and len(response) > 0:\n#         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#             generated_text = response[0]['generated_text'][-1]['content']\n#         else:\n#             generated_text = str(response[0])\n#     else:\n#         generated_text = str(response)\n    \n#     # Extract JSON from response\n#     try:\n#         result = json.loads(generated_text)\n#         if \"questions\" in result and isinstance(result[\"questions\"], list):\n#             # Filter out empty questions and limit to 20\n#             questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#             return questions\n#     except json.JSONDecodeError:\n#         # Try to find JSON pattern in the response\n#         json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n#         if json_match:\n#             result = json.loads(json_match.group(0))\n#             if \"questions\" in result and isinstance(result[\"questions\"], list):\n#                 questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#                 return questions\n\n    \n#     # # Fallback: try to extract questions directly from text\n#     # questions_match = re.search(r'\"questions\":\\s*\\[([^\\]]+)\\]', generated_text)\n#     # if questions_match:\n#     #     questions_str = questions_match.group(1)\n#     #     # Extract individual questions from the list string\n#     #     questions = [q.strip().strip('\"').strip(\"'\") for q in questions_str.split(',') if q.strip()]\n#     #     return questions[:20]\n    \n#     # print(f\"Warning: Could not parse generated questions from response: {generated_text}\")\n#     # return []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gen_q = generate_questions_from_text(pipe, df['story'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T12:10:39.445100Z","iopub.execute_input":"2026-01-03T12:10:39.445417Z","iopub.status.idle":"2026-01-03T12:11:13.506353Z","shell.execute_reply.started":"2026-01-03T12:10:39.445395Z","shell.execute_reply":"2026-01-03T12:11:13.505449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gen_q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T12:12:20.058704Z","iopub.execute_input":"2026-01-03T12:12:20.059451Z","iopub.status.idle":"2026-01-03T12:12:20.064180Z","shell.execute_reply.started":"2026-01-03T12:12:20.059420Z","shell.execute_reply":"2026-01-03T12:12:20.063407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reults_app3 = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for row in range(100):\n#     print(row)\n#     gen_questions = generate_questions_from_text(pipe, df['story'][row])\n\n#     comparisons = {}\n#     remaining_questions = df['questions'].loc[row].tolist()[int(len(df['questions'].loc[row].tolist()) * 0.6):]\n#     for i, q_a in enumerate(gen_questions):\n#         comparisons[q_a] = {}\n#         for j, q_b in enumerate(remaining_questions):\n#             confidence = compare_questions(pipe, q_a, q_b)\n#             comparisons[q_a][q_b] = confidence\n\n#     reults_app3[f'row {row}'] = {\n#     \"gen_questions\": gen_questions,\n#     \"comparisons\": comparisons\n#     }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:51:04.080505Z","iopub.execute_input":"2026-01-03T11:51:04.080843Z","iopub.status.idle":"2026-01-03T11:51:04.089648Z","shell.execute_reply.started":"2026-01-03T11:51:04.080817Z","shell.execute_reply":"2026-01-03T11:51:04.088715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app3_df_last = pd.DataFrame(reults_app3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app3_df_last.to_csv('/kaggle/working/results_app3_df_last.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:52:57.909645Z","iopub.execute_input":"2026-01-03T11:52:57.910069Z","iopub.status.idle":"2026-01-03T11:52:57.917423Z","shell.execute_reply.started":"2026-01-03T11:52:57.910038Z","shell.execute_reply":"2026-01-03T11:52:57.916238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app3 = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for row in rows_100:\n#     print(row)\n#     remaining_questions = df['questions'].loc[row].tolist()[int(len(df['questions'].loc[row].tolist()) * 0.6):]\n#     gen_questions = generate_questions_from_text_with_questions(pipe, df['story'][row], remaining_questions)\n\n#     comparisons = {}\n#     for i, q_a in enumerate(gen_questions):\n#         comparisons[q_a] = {}\n#         for j, q_b in enumerate(remaining_questions):\n#             confidence = compare_questions(pipe, q_a, q_b)\n#             comparisons[q_a][q_b] = confidence\n\n#     results_app3[f'row {row}'] = {\n#     \"gen_questions\": gen_questions,\n#     \"comparisons\": comparisons\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app3_df_new = pd.DataFrame(results_app3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app3_df_new.to_csv('/kaggle/working/results_app3_df_new.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for row in range(20,100):\n#     print(row)\n#     questions = df['questions'].loc[row].tolist()[:int(len(df['questions'].loc[row].tolist()) * 0.6)]\n#     topic = extract_main_topic(pipe, questions)\n#     aspects = extract_topic_aspects(pipe, topic, questions)\n#     gen_questions = generate_questions(pipe, topic, aspects, questions)\n\n#     comparisons = {}\n#     remaining_questions = df['questions'].loc[row].tolist()[int(len(df['questions'].loc[row].tolist()) * 0.6):]\n#     for i, q_a in enumerate(gen_questions):\n#         comparisons[q_a] = {}\n#         for j, q_b in enumerate(remaining_questions):\n#             confidence = compare_questions(pipe, q_a, q_b)\n#             comparisons[q_a][q_b] = confidence\n\n#     results[f'row {row}'] = {\n#     \"topic\": topic,\n#     \"aspects\": aspects,\n#     \"gen_questions\": gen_questions,\n#     \"comparisons\": comparisons\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def extract_text_main_topic(pipe, text: str, temperature: float = 0.7) -> str:\n#     \"\"\"Extract the single main topic/theme from a provided text passage.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a topic extraction expert. Your task is to analyze a text passage and identify the single main topic or central theme that best represents the entire content.\n\n# **Instructions:**\n# 1. Carefully read and understand the entire text passage\n# 2. Identify the core subject, central theme, or primary focus of the text\n# 3. The main topic should be the overarching concept that ties all parts of the text together\n# 4. Output ONLY a JSON object with a single \"main_topic\" field\n# 5. The main topic should be concise (2-5 words maximum) and capture the essence of the text\n# 6. Be specific and accurate - avoid generic terms like \"story\" or \"text\"\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"main_topic\": \"possibility of intelligent life existing on other planets\"\n# }\n# \"\"\"\n    \n#     user_prompt = f\"\"\"\n# **Text passage to analyze:**\n# {text}\n\n# Now identify the single main topic that best represents this entire text:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n\n#     response = pipe(\n#             messages,\n#             temperature=temperature,\n#             do_sample=False,\n#             pad_token_id=pipe.tokenizer.eos_token_id\n#         )\n        \n#     # Extract generated text\n#     if isinstance(response, list) and len(response) > 0:\n#         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#             generated_text = response[0]['generated_text'][-1]['content']\n#         else:\n#             generated_text = str(response[0])\n#     else:\n#         generated_text = str(response)\n    \n#     result = json.loads(generated_text)\n#     if \"main_topic\" in result:\n#         return str(result[\"main_topic\"]).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:49:44.066410Z","iopub.execute_input":"2026-01-04T16:49:44.067015Z","iopub.status.idle":"2026-01-04T16:49:44.073614Z","shell.execute_reply.started":"2026-01-04T16:49:44.066985Z","shell.execute_reply":"2026-01-04T16:49:44.072967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# main_topic = extract_text_main_topic(pipe, df['story'][0])\n# main_topic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:26:58.859279Z","iopub.execute_input":"2026-01-04T16:26:58.859571Z","iopub.status.idle":"2026-01-04T16:27:02.157967Z","shell.execute_reply.started":"2026-01-04T16:26:58.859544Z","shell.execute_reply":"2026-01-04T16:27:02.157357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reults_main_topics = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:50:02.952521Z","iopub.execute_input":"2026-01-04T16:50:02.952834Z","iopub.status.idle":"2026-01-04T16:50:02.956340Z","shell.execute_reply.started":"2026-01-04T16:50:02.952806Z","shell.execute_reply":"2026-01-04T16:50:02.955742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for row in rows_1000:\n#     if row % 10 == 0:\n#         print(row)\n#     main_topic = extract_text_main_topic(pipe, df['story'][row])\n\n#     reults_main_topics[f'row {row}'] = {\n#     \"main_topic\": main_topic\n#     }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:50:03.148838Z","iopub.execute_input":"2026-01-04T16:50:03.149086Z","iopub.status.idle":"2026-01-04T16:50:28.809976Z","shell.execute_reply.started":"2026-01-04T16:50:03.149063Z","shell.execute_reply":"2026-01-04T16:50:28.809378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reults_main_topics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:51:21.606777Z","iopub.execute_input":"2026-01-04T16:51:21.607487Z","iopub.status.idle":"2026-01-04T16:51:21.611837Z","shell.execute_reply.started":"2026-01-04T16:51:21.607456Z","shell.execute_reply":"2026-01-04T16:51:21.611186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reults_main_topics_df = pd.DataFrame(reults_main_topics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:50:45.603398Z","iopub.execute_input":"2026-01-04T16:50:45.604141Z","iopub.status.idle":"2026-01-04T16:50:45.610276Z","shell.execute_reply.started":"2026-01-04T16:50:45.604109Z","shell.execute_reply":"2026-01-04T16:50:45.609644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reults_main_topics_df.T.to_csv('/kaggle/working/reults_main_topics_df.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:51:27.846057Z","iopub.execute_input":"2026-01-04T16:51:27.846736Z","iopub.status.idle":"2026-01-04T16:51:27.857867Z","shell.execute_reply.started":"2026-01-04T16:51:27.846704Z","shell.execute_reply":"2026-01-04T16:51:27.857190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reults_main_topics_df = pd.read_csv('/kaggle/input/reults-main-topics-df/reults_main_topics_df.csv')\n# all_topics = reults_main_topics_df['main_topic'].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:37:18.966433Z","iopub.execute_input":"2026-01-05T13:37:18.966821Z","iopub.status.idle":"2026-01-05T13:37:19.064652Z","shell.execute_reply.started":"2026-01-05T13:37:18.966791Z","shell.execute_reply":"2026-01-05T13:37:19.064173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_main_topics_df = pd.read_csv('/kaggle/input/1000-topics-20q/reults_main_topics_df.csv')\nall_topics = results_main_topics_df['main_topic'].tolist()\nrow_identifiers = results_main_topics_df['Unnamed: 0'].str.extract(r'row\\s*(\\d+)')[0].astype(int).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:36.902649Z","iopub.execute_input":"2026-01-18T04:44:36.902977Z","iopub.status.idle":"2026-01-18T04:44:44.792727Z","shell.execute_reply.started":"2026-01-18T04:44:36.902940Z","shell.execute_reply":"2026-01-18T04:44:44.791859Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import DBSCAN\nimport numpy as np\nfrom scipy.spatial.distance import cosine\n\ndef cluster_topics_by_embeddings(topics_list, row_identifiers, min_cluster_size=3, eps=0.4):\n    \"\"\"\n    Cluster topics using sentence embeddings and DBSCAN\n    Returns: dict mapping cluster_id to list of (dialogue_index, topic)\n    \"\"\"\n    # Load pre-trained model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    # Generate embeddings\n    embeddings = model.encode(topics_list, show_progress_bar=True)\n    \n    # Cluster using DBSCAN (good for varying cluster sizes)\n    clustering = DBSCAN(eps=eps, min_samples=min_cluster_size, metric='cosine')\n    cluster_labels = clustering.fit_predict(embeddings)\n    \n    # Organize results\n    clusters = {}\n    for idx, (label, topic) in enumerate(zip(cluster_labels, topics_list)):\n        row_id = row_identifiers[idx] \n        if label == -1:  # Noise points\n            continue\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append((row_id, topic))\n    \n    return clusters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:44.793926Z","iopub.execute_input":"2026-01-18T04:44:44.794248Z","iopub.status.idle":"2026-01-18T04:44:50.249167Z","shell.execute_reply.started":"2026-01-18T04:44:44.794215Z","shell.execute_reply":"2026-01-18T04:44:50.248564Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"clusters = cluster_topics_by_embeddings(all_topics, row_identifiers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:50.250035Z","iopub.execute_input":"2026-01-18T04:44:50.250682Z","iopub.status.idle":"2026-01-18T04:44:53.522707Z","shell.execute_reply.started":"2026-01-18T04:44:50.250656Z","shell.execute_reply":"2026-01-18T04:44:53.522019Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1663710f96f421fa201c49f5c09fa11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e373e4d9a7c34cd1bd8e84976a84a2a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a0f5e29b1e4493accea78e4284511f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49b8ab592d2a43198a691988618d1bf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea0b3c0cd9e94fde9c9822d94856379a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28f0990841d6447b8d7441630fb59b6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651bc9cab1e045b6bfd5c0953b968dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b20565c21bf54ff4b180fe4938c8acc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b859bba5186a42a0adad28f26c3063c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9016bc209349458ef0d19651141889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b320bbf435465e94e852dbfa76a462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a03ae97a86491690f55c95c8c6f1c3"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# clusters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:37:58.365009Z","iopub.execute_input":"2026-01-18T04:37:58.365314Z","iopub.status.idle":"2026-01-18T04:37:58.376260Z","shell.execute_reply.started":"2026-01-18T04:37:58.365279Z","shell.execute_reply":"2026-01-18T04:37:58.375440Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{np.int64(0): [(95, 'family life and recent events'),\n  (321, 'reconciliation of justice and fate'),\n  (407, 'family interaction and responsibility'),\n  (489, \"brothers' relationship and dynamics\"),\n  (587, 'family secrets and silent tensions'),\n  (656, 'Family dynamics and social interactions'),\n  (910, 'Family dynamics at a water-party'),\n  (1047, 'Social dynamics at a dinner party'),\n  (1707, 'Family life and relationships'),\n  (1717, 'dinner arrangement mystery'),\n  (1764, 'family bonding and communication'),\n  (2187, 'family conflict and reconciliation'),\n  (2422, 'social dynamics among women'),\n  (2667, \"Brother and sister's different behaviors\"),\n  (2769, 'social dynamics at a party'),\n  (2960, 'family dynamics and social status')],\n np.int64(1): [(165, 'young adult fiction books'),\n  (464, 'popular young adult literature'),\n  (515, \"famous children's books\")],\n np.int64(2): [(233, \"Thomas Edison's inventions and life\"),\n  (1126, \"Ford and Edison's friendship\"),\n  (1507, \"Tesla's electrical inventions\"),\n  (1626, 'inventors and their contributions')],\n np.int64(8): [(344, 'tribal peoples and their status'),\n  (1056, 'Inuit people and culture'),\n  (2445, 'Eskimo life in the Arctic')],\n np.int64(14): [(447, 'search for lost children'),\n  (1446, \"boys' search for a missing person\"),\n  (1699, 'search for missing autistic teen')],\n np.int64(3): [(453, 'intelligent life on other planets'),\n  (460, 'intelligent life on other planets'),\n  (551, 'intelligent life on other planets')],\n np.int64(4): [(476, 'Republican debate strategies'),\n  (1186, 'presidential debate analysis'),\n  (2770, '2016 Republican presidential debates')],\n np.int64(5): [(522, 'freedom of enslaved people'),\n  (1832, 'love and freedom'),\n  (2182, 'slave trade and resistance'),\n  (2917, 'love and sacrifice')],\n np.int64(6): [(590, 'Friendship across cultures'),\n  (861, 'acts of kindness and sharing'),\n  (864, 'cross-species friendship'),\n  (1027, 'kindness of a young girl'),\n  (1063, 'friendship and kindness'),\n  (1399, 'helping others through friendship'),\n  (2041, 'acts of kindness and human connection'),\n  (2082, 'love across cultural differences'),\n  (2132, 'small acts of kindness'),\n  (2196, 'making friends through kindness'),\n  (2767, 'kindness and human connection')],\n np.int64(9): [(685, 'military corruption scandal'),\n  (1135, 'political corruption'),\n  (2831, 'border agent corruption')],\n np.int64(12): [(764, \"Bhutan's political and cultural history\"),\n  (1279, \"Cambodia's history and culture\"),\n  (2134, 'history of Laos')],\n np.int64(7): [(802, 'Tennis matches and player performances'),\n  (885, \"Grigor Dimitrov's Wimbledon breakthrough\"),\n  (2531, \"Federer's Wimbledon victory\"),\n  (2972, 'Tennis tournament results')],\n np.int64(15): [(1020, \"A student's struggle and dreams\"),\n  (1569, 'pursuit of dreams'),\n  (2055, 'dreams coming true through persistence'),\n  (2314, \"pursuing one's dreams\")],\n np.int64(10): [(1216, 'overcoming failure to succeed'),\n  (1584, 'overcoming adversity through talent and love'),\n  (1706, 'overcoming adversity to achieve academic success')],\n np.int64(11): [(1239, 'secret letter discovery'),\n  (1245, 'Secret discovery and celebration'),\n  (1301, 'secret letter and betrayal')],\n np.int64(13): [(1355, \"Norway's geography and government\"),\n  (1979, \"Switzerland's geography and government\"),\n  (2654, \"Norway's geography and government\")],\n np.int64(17): [(1491, 'trust in military strategy'),\n  (2006, 'military confrontation and strategy'),\n  (2176, 'Military surrender and troop movements')],\n np.int64(16): [(1540, 'Scout journey into the woods'),\n  (1640, 'journey through the forest'),\n  (1848, 'evening in the forest')],\n np.int64(19): [(1594, \"Fernando Alonso's victory at Spanish Grand Prix\"),\n  (2431, 'MotoGP race results and standings'),\n  (2622, \"Marc Marquez's MotoGP victory\")],\n np.int64(18): [(2146, 'British music industry association'),\n  (2544, 'British music industry association'),\n  (3040, 'Spanish music industry organization')]}"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# def extract_cluster_questions(df, cluster_dialogues):\n#     combined_questions = []\n    \n#     for dialogue_idx, topic in cluster_dialogues:\n#         # Get all questions for this dialogue\n#         all_questions = df['questions'].loc[dialogue_idx].tolist()\n        \n#         # Calculate 60% cutoff\n#         cutoff_idx = int(len(all_questions) * 0.6)\n        \n#         # Extract first 60% of questions\n#         first_60_percent = all_questions[:cutoff_idx]\n        \n#         # Add to combined list\n#         combined_questions.extend(first_60_percent)\n    \n#     return combined_questions\n\n# def extract_cluster_last_questions(df, cluster_dialogues):\n#     combined_questions = []\n    \n#     for dialogue_idx, topic in cluster_dialogues:\n#         # Get all questions for this dialogue\n#         all_questions = df['questions'].loc[dialogue_idx].tolist()\n        \n#         # Calculate starting index for last 60% (skip first 40%)\n#         start_idx = int(len(all_questions) * 0.6)\n        \n#         # Extract last 60% of questions\n#         last_60_percent = all_questions[start_idx:]\n        \n#         # Add to combined list\n#         combined_questions.extend(last_60_percent)\n    \n#     return combined_questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:02:02.270765Z","iopub.execute_input":"2026-01-05T12:02:02.271417Z","iopub.status.idle":"2026-01-05T12:02:02.276654Z","shell.execute_reply.started":"2026-01-05T12:02:02.271390Z","shell.execute_reply":"2026-01-05T12:02:02.275966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_cluster_questions(df, cluster_dialogues):\n    cluster_qs = {}\n    \n    for dialogue_idx, topic in cluster_dialogues:\n        # Get all questions for this dialogue\n        all_questions = df['questions'].loc[dialogue_idx].tolist()\n        \n        # Calculate 60% cutoff\n        cutoff_idx = int(len(all_questions) * 0.6)\n        \n        # Extract first 60% of questions\n        first_60_percent = all_questions[:cutoff_idx]\n        \n        # Add to combined list\n        cluster_qs[dialogue_idx] = {\n            'first_60_percent': first_60_percent\n        }\n    \n    return cluster_qs\n\ndef extract_cluster_last_questions(df, cluster_dialogues):\n    cluster_qs_test = {}\n    \n    for dialogue_idx, topic in cluster_dialogues:\n        # Get all questions for this dialogue\n        all_questions = df['questions'].loc[dialogue_idx].tolist()\n        \n        # Calculate starting index for last 60% (skip first 40%)\n        start_idx = int(len(all_questions) * 0.6)\n        \n        # Extract last 60% of questions\n        last_40_percent = all_questions[start_idx:]\n        \n        # Add to combined list\n        cluster_qs_test[dialogue_idx] = {\n            'last_40_percent': last_40_percent\n        }\n    \n    return cluster_qs_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:49:26.549445Z","iopub.execute_input":"2026-01-18T04:49:26.549762Z","iopub.status.idle":"2026-01-18T04:49:26.556663Z","shell.execute_reply.started":"2026-01-18T04:49:26.549735Z","shell.execute_reply":"2026-01-18T04:49:26.555551Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# def extract_cluster_aspects(pipe, cluster_topics: list[str], combined_questions: list[str], temperature: float = 0.7) -> list[str]:\n#     \"\"\"Extract common aspects/details of interest from a cluster of similar topics and their combined questions.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a cluster analysis expert. Your task is to analyze a group of semantically similar topics and their associated questions to identify the key aspects, details, or dimensions that users are consistently interested in across this topic cluster.\n\n# **Instructions:**\n# 1. You are given multiple similar topics (from the same cluster) and combined questions from all dialogues in this cluster\n# 2. The topics are semantically related and represent the same general domain/subject area\n# 3. Analyze the questions to identify recurring themes, specific details, or sub-topics that users frequently ask about\n# 4. Focus on aspects that appear across multiple topics/questions in the cluster\n# 5. Output ONLY a JSON object with a single \"aspects\" field containing a list of strings\n# 6. Each aspect should be concise (2-6 words) and represent a distinct dimension of interest\n# 7. Include only aspects that are clearly supported by the questions and topics\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   aspects: [\"scientific reasoning for existence\", \"detection methods and technology\", \"expert predictions and timeline\"]\n# }\n\n# **Examples:**\n# - Topic: \"possibility of intelligent life existing on other planets\" + Questions about evidence, which planet, methods, timeline, and experts → [age of universe, Earth-like planets,  radio telescopes, timeline for contact]\n# - Topic: \"university\" + Questions about founders, buildings, programs → [\"founding history\", \"campus architecture\", \"academic programs\"]\n# \"\"\"\n    \n#     topics_formatted = \"\\n\".join([f\"- \\\"{topic}\\\"\" for _, topic in cluster_topics])\n#     questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(combined_questions)])\n    \n#     user_prompt = f\"\"\"\n# **Cluster of Similar Topics:**\n# {topics_formatted}\n\n# **Combined Questions from All Dialogues in This Cluster:**\n# {questions_formatted}\n\n# Now analyze this topic cluster and extract the key aspects or details that users are consistently interested in across these related topics:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n    \n#     response = pipe(\n#         messages,\n#         temperature=temperature,\n#         do_sample=False,\n#         pad_token_id=pipe.tokenizer.eos_token_id\n#     )\n    \n#     # Extract generated text\n#     if isinstance(response, list) and len(response) > 0:\n#         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#             generated_text = response[0]['generated_text'][-1]['content']\n#         else:\n#             generated_text = str(response[0])\n#     else:\n#         generated_text = str(response)\n    \n#     # Extract JSON from response\n#     result = json.loads(generated_text)\n#     if \"aspects\" in result and isinstance(result[\"aspects\"], list):\n#         return [str(aspect).strip() for aspect in result[\"aspects\"] if aspect]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:02:02.277849Z","iopub.execute_input":"2026-01-05T12:02:02.278278Z","iopub.status.idle":"2026-01-05T12:02:04.922673Z","shell.execute_reply.started":"2026-01-05T12:02:02.278235Z","shell.execute_reply":"2026-01-05T12:02:04.921948Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def generate_cluster_questions(pipe, cluster_topics: list[str], aspects: list[str], asked_questions: list[str], temperature: float = 0.7) -> list[str]:\n#     \"\"\"Generate possible follow-up questions based on a cluster of similar topics, their aspects, and already asked questions.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a question generation expert specializing in cluster analysis. Your task is to generate new, relevant questions that users might ask across a cluster of semantically similar topics, based on identified aspects of interest and avoiding questions that have already been asked.\n\n# **Instructions:**\n# 1. You are given:\n#    - Multiple similar topics (from the same semantic cluster)\n#    - Key aspects identified as common interests across this topic cluster\n#    - Questions that have already been asked across all dialogues in this cluster\n# 2. Generate 8 NEW questions that:\n#    - Are relevant to the overall domain/theme represented by the cluster\n#    - Cover the specified aspects from multiple angles and perspectives\n#    - Are fundamentally different from already asked questions (avoid rephrasing the same concepts)\n#    - Sound natural and like something real users would ask across different conversations\n#    - Explore connections between topics in the cluster or dive deeper into under-explored aspects\n# 3. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n# 4. Each question should be clear, concise, self-contained, and specific\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"questions\": [\n#     \"What galaxy do we live in?\",\n#     \"How many stars does it have?\",\n#     \"Does Shostak believe beings from space could make contact with Earth soon?\"\n#   ]\n# }\n\n# **Critical Guidelines:**\n# - DO NOT repeat or rephrase any concepts from the already asked questions\n# - DO explore aspects from fresh perspectives that haven't been covered yet\n# - DO consider how different topics in the cluster might intersect or relate\n# - DO make questions specific and actionable rather than vague\n# - DO maintain natural, conversational language appropriate for the domain\n# \"\"\"\n    \n#     topics_formatted = \"\\n\".join([f\"- Topic {i+1}: \\\"{topic}\\\"\" for i, topic in enumerate(cluster_topics)])\n#     aspects_formatted = \", \".join([f'\"{aspect}\"' for aspect in aspects])\n#     asked_questions_formatted = \"\\n\".join([f\"- \\\"{q}\\\"\" for q in asked_questions])\n    \n#     user_prompt = f\"\"\"\n# **Cluster of Similar Topics:**\n# {topics_formatted}\n\n# **Key Aspects of Interest Across This Cluster:**\n# {aspects_formatted}\n\n# **Questions Already Asked Across All Dialogues in This Cluster:**\n# {asked_questions_formatted}\n\n# Now generate new, relevant questions that users might ask. These questions should explore the cluster domain and aspects in novel ways, avoiding repetition of the already asked questions:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n\n#     try:\n#         response = pipe(\n#             messages,\n#             temperature=temperature,\n#             do_sample=True,\n#             pad_token_id=pipe.tokenizer.eos_token_id\n#         )\n        \n#         # Extract generated text\n#         if isinstance(response, list) and len(response) > 0:\n#             if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#                 generated_text = response[0]['generated_text'][-1]['content']\n#             else:\n#                 generated_text = str(response[0])\n#         else:\n#             generated_text = str(response)\n        \n#         # Extract JSON from response\n#         result = json.loads(generated_text)\n#         if \"questions\" in result and isinstance(result[\"questions\"], list):\n#             return [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n#     except json.JSONDecodeError:\n#         # Try to find JSON pattern in the response\n#         json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n#         if json_match:\n#             result = json.loads(json_match.group(0))\n#             if \"questions\" in result and isinstance(result[\"questions\"], list):\n#                 return [str(q).strip() for q in result[\"questions\"] if q and q.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:02:04.923791Z","iopub.execute_input":"2026-01-05T12:02:04.924234Z","iopub.status.idle":"2026-01-05T12:02:04.937828Z","shell.execute_reply.started":"2026-01-05T12:02:04.924158Z","shell.execute_reply":"2026-01-05T12:02:04.937007Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def batch_compare_questions(pipe, question_pairs: list[tuple[str, str]], \n#                            batch_size: int = 10, temperature: float = 0.7, \n#                            max_new_tokens: int = 1300) -> dict[tuple[str, str], float]:\n#     \"\"\"\n#     Compare multiple question pairs in batches to speed up processing.\n    \n#     Args:\n#         question_pairs: List of (question1, question2) tuples to compare\n#         batch_size: Number of pairs to process in each batch\n#         temperature: Sampling temperature for generation\n#         max_new_tokens: Maximum tokens to generate per batch\n    \n#     Returns:\n#         Dictionary mapping (q1, q2) tuples to confidence scores\n#     \"\"\"\n#     results = {}\n#     print(len(question_pairs))\n    \n#     # Process in batches\n#     for batch_start in range(0, len(question_pairs), batch_size):\n#         batch_end = min(batch_start + batch_size, len(question_pairs))\n#         batch_pairs = question_pairs[batch_start:batch_end]\n        \n#         # Create batch prompt\n#         sys_prompt = \"\"\"\n# You are a semantic equivalence evaluator. Your task is to compare multiple pairs of questions and determine the confidence that each pair is semantically equivalent.\n\n# **Instructions:**\n# 1. For each question pair, compare the meaning and intent\n# 2. Determine if they're asking for the same information and would yield the same answer\n# 3. Output ONLY a JSON object with a \"results\" field containing a list of objects\n# 4. Each object must have \"question1\", \"question2\", and \"confidence\" fields\n# 5. Confidence must be a number between 0 and 1\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"results\": [\n#     {\"question1\": \"q1 text\", \"question2\": \"q2 text\", \"confidence\": 0.85},\n#     {\"question1\": \"q1 text\", \"question2\": \"q2 text\", \"confidence\": 0.32}\n#   ]\n# }\n\n# **Explanation of confidence scores:**\n# - 0.9-1.0: Definitely equivalent (same meaning, intent, and expected answer)\n# - 0.7-0.9: Highly likely equivalent (minor wording differences only)\n# - 0.5-0.7: Possibly equivalent (similar intent but different phrasing)\n# - 0.3-0.5: Unlikely equivalent (related but different aspects)\n# - 0.0-0.3: Definitely different (completely different questions)\n# \"\"\"\n        \n#         # Format batch pairs\n#         pairs_formatted = []\n#         for i, (q1, q2) in enumerate(batch_pairs):\n#             pairs_formatted.append(f\"Pair {i+1}:\\nQuestion 1: \\\"{q1}\\\"\\nQuestion 2: \\\"{q2}\\\"\\n\")\n        \n#         user_prompt = f\"\"\"\n# **Question pairs to compare:**\n# {chr(10).join(pairs_formatted)}\n\n# Now evaluate all these question pairs and provide confidence scores for each:\n# \"\"\"\n        \n#         messages = [\n#             {\"role\": \"system\", \"content\": sys_prompt.strip()},\n#             {\"role\": \"user\", \"content\": user_prompt.strip()}\n#         ]\n        \n#         try:\n#             # Single LLM call for the entire batch\n#             response = pipe(\n#                 messages,\n#                 temperature=temperature,\n#                 max_new_tokens = max_new_tokens,\n#                 do_sample=False,\n#                 pad_token_id=pipe.tokenizer.eos_token_id\n#             )\n            \n#             # Extract generated text\n#             generated_text = response[0]['generated_text'][-1]['content'] if isinstance(response, list) else str(response)\n            \n#             # Parse JSON response\n#             try:\n#                 batch_results = json.loads(generated_text)\n#                 if \"results\" in batch_results and isinstance(batch_results[\"results\"], list):\n#                     for item in batch_results[\"results\"]:\n#                         if all(key in item for key in [\"question1\", \"question2\", \"confidence\"]):\n#                             q1 = item[\"question1\"].strip()\n#                             q2 = item[\"question2\"].strip()\n#                             confidence = float(item[\"confidence\"])\n#                             results[(q1, q2)] = max(0.0, min(1.0, confidence))\n#             except (json.JSONDecodeError, ValueError) as e:\n#                 print(f\"Error parsing batch results: {e}\")\n#                 print(f\"Raw response: {generated_text}\")\n#         except Exception as e:\n#             print(f\"Error in batch comparison: {e}\")\n\n#     return results\n\n# #         try:\n# #             response = pipe(\n# #                 messages,\n# #                 temperature=temperature,\n# #                 max_new_tokens=max_new_tokens,\n# #                 do_sample=False,\n# #                 pad_token_id=pipe.tokenizer.eos_token_id\n# #             )\n            \n# #             generated_text = response[0]['generated_text'][-1]['content'] if isinstance(response, list) else str(response)\n            \n# #             # Try to parse JSON with multiple fallback strategies\n# #             batch_results = parse_json_with_fallbacks(generated_text, batch_pairs)\n            \n# #             if batch_results and \"results\" in batch_results and isinstance(batch_results[\"results\"], list):\n# #                 for item in batch_results[\"results\"]:\n# #                     try:\n# #                         if all(key in item for key in [\"question1\", \"question2\", \"confidence\"]):\n# #                             q1 = str(item[\"question1\"]).strip()\n# #                             q2 = str(item[\"question2\"]).strip()\n# #                             confidence = float(item[\"confidence\"])\n# #                             results[(q1, q2)] = max(0.0, min(1.0, confidence))\n# #                     except (ValueError, TypeError) as e:\n# #                         print(f\"Error processing individual result: {e}\")\n# #                         print(f\"Problematic item: {item}\")\n# #             else:\n# #                 print(f\"Invalid batch results structure: {batch_results}\")\n                \n# #         except Exception as e:\n# #             print(f\"Error in batch comparison: {e}\")\n# #             print(f\"Raw response that caused error: {generated_text}\")\n    \n# #     return results\n\n# # def parse_json_with_fallbacks(json_text: str, expected_pairs: list) -> dict:\n# #     \"\"\"\n# #     Attempt to parse JSON with multiple fallback strategies for common LLM errors.\n# #     \"\"\"\n# #     # Strategy 1: Try direct parsing\n# #     try:\n# #         return json.loads(json_text)\n# #     except json.JSONDecodeError as e1:\n# #         print(f\"Direct JSON parse failed: {e1}\")\n    \n# #     # Strategy 2: Extract JSON-like content using regex\n# #     try:\n# #         # Look for content between { and }\n# #         json_match = re.search(r'\\{[\\s\\S]*\\}', json_text)\n# #         if json_match:\n# #             extracted_json = json_match.group(0)\n# #             try:\n# #                 return json.loads(extracted_json)\n# #             except json.JSONDecodeError as e2:\n# #                 print(f\"Regex extraction parse failed: {e2}\")\n# #     except Exception as e2:\n# #         print(f\"Regex extraction failed: {e2}\")\n    \n# #     # Strategy 3: Fix common JSON errors manually\n# #     try:\n# #         # Fix the specific error seen in the example: extra } before confidence\n# #         fixed_text = re.sub(r'\"}\\s*,\\s*\"confidence\"', r'\", \"confidence\"', json_text)\n# #         # Fix missing quotes around confidence values\n# #         fixed_text = re.sub(r':\\s*(\\d+\\.\\d+)}', r': \"\\1\"}', fixed_text)\n# #         # Fix trailing commas\n# #         fixed_text = re.sub(r',\\s*}', '}', fixed_text)\n# #         fixed_text = re.sub(r',\\s*\\]', ']', fixed_text)\n        \n# #         try:\n# #             return json.loads(fixed_text)\n# #         except json.JSONDecodeError as e3:\n# #             print(f\"Manual fixing parse failed: {e3}\")\n# #     except Exception as e3:\n# #         print(f\"Manual fixing failed: {e3}\")\n    \n# #     # Strategy 4: Last resort - try to extract confidence values manually\n# #     try:\n# #         results = []\n# #         for i, (q1, q2) in enumerate(expected_pairs):\n# #             # Look for confidence values in the text\n# #             pattern = f'\"{q1}\"[^{{}}]*\"{q2}\"[^{{}}]*\"confidence\":\\s*([\\d\\.]+)'\n# #             match = re.search(pattern, json_text, re.IGNORECASE | re.DOTALL)\n# #             if match:\n# #                 confidence = float(match.group(1))\n# #                 results.append({\n# #                     \"question1\": q1,\n# #                     \"question2\": q2,\n# #                     \"confidence\": confidence\n# #                 })\n        \n# #         if results:\n# #             return {\"results\": results}\n# #     except Exception as e4:\n# #         print(f\"Manual extraction failed: {e4}\")\n    \n# #     print(\"All JSON parsing strategies failed. Returning empty results.\")\n# #     return {\"results\": []}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:35:01.775010Z","iopub.execute_input":"2026-01-05T13:35:01.775601Z","iopub.status.idle":"2026-01-05T13:35:01.786743Z","shell.execute_reply.started":"2026-01-05T13:35:01.775573Z","shell.execute_reply":"2026-01-05T13:35:01.786160Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_aspects_cluster(pipe, current_topic: str, current_questions: list[str], cluster_questions: list[list[str]], temperature: float = 0.7) -> list[str]:\n    \"\"\"Extract detailed aspects of a topic using context from all dialogues in the same cluster.\"\"\"\n    \n    system_prompt = \"\"\"\nYou are an aspect extraction expert specializing in contextual analysis. Your task is to identify specific details and dimensions of a topic by analyzing questions from:\n1. The current dialogue (primary focus)\n2. Related dialogues in the same topic cluster (providing broader context)\n\n**Instructions:**\n1. Analyze ALL provided questions together - they all relate to the same core topic cluster\n2. Identify specific aspects, details, or dimensions of the main topic that users are interested in\n3. Focus on concrete details mentioned or implied across the dialogues\n4. Prioritize aspects that appear in multiple dialogues OR are deeply explored in the current dialogue\n5. Output ONLY a JSON object with a single \"aspects\" field containing a list of strings\n6. Each aspect should be concise (2-6 words) and represent a distinct detail or sub-topic\n7. Include only aspects strongly supported by the questions\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"aspects\": [\"album release dates\", \"auction bidding processes\", \"memorabilia authentication\"]\n}\n\n**Critical Guidelines:**\n- LEVERAGE CLUSTER CONTEXT: Use questions from related dialogues to discover aspects not explicitly mentioned in the current dialogue\n- AVOID GENERIC ASPECTS: Be specific and concrete (e.g., \"vinyl condition grading\" not just \"records\")\n- PRIORITIZE RECURRING THEMES: Aspects appearing across multiple dialogues are more significant\n- MAINTAIN TOPIC FOCUS: All aspects must directly relate to the core topic cluster\n\"\"\"\n    \n    # Format current dialogue questions\n    current_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(current_questions)])  # Limit to 10 for token efficiency\n    \n    # Format cluster context questions (from other dialogues)\n    cluster_context = []\n    for i, dialogue_questions in enumerate(cluster_questions[:3]):  # Limit to 3 dialogues\n        dialogue_sample = dialogue_questions  # Limit to 5 questions per dialogue\n        cluster_context.append(f\"Related Dialogue {i+1} questions:\\n\" + \"\\n\".join([f\"  • \\\"{q}\\\"\" for q in dialogue_sample]))\n    cluster_formatted = \"\\n\\n\".join(cluster_context)\n    \n    user_prompt = f\"\"\"\n**CORE TOPIC CLUSTER:** \"{current_topic}\"\n\n**CURRENT DIALOGUE QUESTIONS (primary focus):**\n{current_formatted}\n\n**ADDITIONAL CONTEXT FROM RELATED DIALOGUES (same topic cluster):**\n{cluster_formatted}\n\nAnalyze ALL questions above to extract the most significant specific aspects of \"{current_topic}\" that users care about:\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n\n    response = pipe(\n        messages,\n        temperature=temperature,\n        do_sample=False,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n    \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n    \n    # Extract JSON from response\n    result = json.loads(generated_text)\n    if \"aspects\" in result and isinstance(result[\"aspects\"], list):\n        return [str(aspect).strip() for aspect in result[\"aspects\"] if aspect]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:53.531635Z","iopub.execute_input":"2026-01-18T04:44:53.532049Z","iopub.status.idle":"2026-01-18T04:44:53.562300Z","shell.execute_reply.started":"2026-01-18T04:44:53.532019Z","shell.execute_reply":"2026-01-18T04:44:53.561503Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# cluster_questions_dict = extract_cluster_questions(df, clusters[1])\n# clusters[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T05:01:11.211174Z","iopub.execute_input":"2026-01-18T05:01:11.211682Z","iopub.status.idle":"2026-01-18T05:01:11.217554Z","shell.execute_reply.started":"2026-01-18T05:01:11.211651Z","shell.execute_reply":"2026-01-18T05:01:11.216785Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[(165, 'young adult fiction books'),\n (464, 'popular young adult literature'),\n (515, \"famous children's books\")]"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# dialogue_aspects = {}\n# for dialogue_idx, topic in clusters[1]:\n#         # Get current dialogue questions\n#         current_questions = cluster_questions_dict[dialogue_idx]['first_60_percent']\n        \n#         # Prepare cluster context: questions from OTHER dialogues in cluster\n#         cluster_context_questions = [\n#             cluster_questions_dict[other_idx]['first_60_percent'] \n#             for other_idx in cluster_questions_dict.keys() \n#             if other_idx != dialogue_idx\n#         ]\n\n#         dialogue_aspects[dialogue_idx] = extract_aspects_cluster(\n#             pipe,\n#             current_topic=topic,\n#             current_questions=current_questions,\n#             cluster_questions=cluster_context_questions\n#         )\n# cluster_results = {\"dialogues\": {}}\n\n# for dialogue_idx, topic in clusters[1]:\n#     # Get current dialogue data\n#     current_questions = cluster_questions_dict[dialogue_idx]['first_60_percent']\n#     current_aspects = dialogue_aspects[dialogue_idx]\n    \n#     # Prepare cluster context for generation: aspects + questions from OTHER dialogues\n#     cluster_context = []\n#     for other_idx, other_topic in clusters[1]:\n#         if other_idx != dialogue_idx:\n#             cluster_context.append({\n#                 'aspects': dialogue_aspects[other_idx],\n#                 'questions': cluster_questions_dict[other_idx]['first_60_percent']\n#             })\n    \n#     # Generate questions using enriched context\n#     gen_questions = generate_questions_cluster(\n#         pipe,\n#         current_topic=topic,\n#         current_aspects=current_aspects,\n#         current_questions=current_questions,\n#         cluster_context=cluster_context\n#     )\n#     print(f\"  Generated {len(gen_questions)} questions for dialogue {dialogue_idx}\")\n    \n#     # Get remaining questions (last 40%) for THIS dialogue only\n#     remaining_questions = extract_cluster_last_questions(df, [(dialogue_idx, topic)])\n#     dialogue_remaining = remaining_questions[dialogue_idx]['last_40_percent']\n    \n#     # Compare generated questions with remaining questions\n#     comparisons = {}\n#     for q_a in gen_questions:\n#         comparisons[q_a] = {}\n#         for q_b in dialogue_remaining:\n#             confidence = compare_questions(pipe, q_a, q_b)\n#             comparisons[q_a][q_b] = confidence\n    \n#     # Store results for this dialogue\n#     cluster_results[\"dialogues\"][dialogue_idx] = {\n#         \"topic\": topic,\n#         \"aspects\": current_aspects,\n#         \"gen_questions\": gen_questions,\n#         \"comparisons\": comparisons\n#     }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T05:01:12.024968Z","iopub.execute_input":"2026-01-18T05:01:12.025493Z","iopub.status.idle":"2026-01-18T05:07:33.100318Z","shell.execute_reply.started":"2026-01-18T05:01:12.025463Z","shell.execute_reply":"2026-01-18T05:07:33.099693Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"  Generated 8 questions for dialogue 165\n  Generated 8 questions for dialogue 464\n  Generated 8 questions for dialogue 515\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def generate_questions_cluster(pipe, current_topic: str, current_aspects: list[str], current_questions: list[str], cluster_context: list[dict], temperature: float = 0.7, max_new_tokens: int = 500) -> list[str]:\n    \"\"\"Generate context-aware questions for a dialogue using enriched cluster information.\"\"\"\n    \n    system_prompt = \"\"\"\nYou are an expert question generator specializing in contextual dialogue expansion. Your task is to generate new, relevant questions for a user by leveraging:\n1. The current dialogue's specific focus (primary priority)\n2. Broader context from related dialogues in the same topic cluster\n\n**Instructions:**\n1. Analyze ALL provided information about the topic cluster\n2. Generate 8 NEW questions that:\n   - Are highly relevant to the CURRENT dialogue's specific aspects and questions\n   - Incorporate valuable details discovered from OTHER dialogues in the cluster\n   - Avoid repeating or rephrasing questions already asked in the current dialogue\n   - Explore unexplored dimensions of the topic revealed by the cluster context\n   - Sound natural and conversational (like a real user would ask)\n3. Prioritize questions that:\n   - Bridge the current dialogue's focus with complementary aspects from cluster context\n   - Address obvious gaps where cluster context reveals important aspects missing in current dialogue\n   - Maintain specificity to the current dialogue's unique angle within the broader topic\n4. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n5. Each question must be clear, concise, and self-contained\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"questions\": [\n    \"What authentication methods are used for rare Michael Jackson memorabilia?\",\n    \"How do posthumous album releases affect the value of original memorabilia?\",\n    \"What was the highest bid ever recorded for Jackson's signature glove?\"\n  ]\n}\n\n**Critical Guidelines:**\n- CURRENT DIALOGUE IS PRIMARY: Never generate questions irrelevant to the current dialogue's specific focus\n- CLUSTER CONTEXT IS ENRICHMENT: Use other dialogues to discover NEW angles, not to change the core topic\n- AVOID REPETITION: Do not generate questions semantically equivalent to already asked questions\n- BE SPECIFIC: Leverage concrete details from cluster aspects\n- MAINTAIN CONVERSATIONAL FLOW: Questions should feel like natural follow-ups to the current discussion\n\"\"\"\n    \n    # Format current dialogue information\n    current_aspects_formatted = \", \".join([f'\"{a}\"' for a in current_aspects])\n    current_questions_formatted = \"\\n\".join([f\"- \\\"{q}\\\"\" for q in current_questions])\n    \n    # Format cluster context (only include relevant parts from other dialogues)\n    cluster_context_formatted = []\n    for i, context in enumerate(cluster_context[:3]):  # Limit to 2 other dialogues for token efficiency\n        aspects = context.get('aspects', [])\n        questions = context.get('questions', [])\n        \n        if aspects or questions:\n            dialogue_context = f\"Related Dialogue {i+1}:\\n\"\n            if aspects:\n                dialogue_context += f\"Key aspects: {', '.join([f'\\\"{a}\\\"' for a in aspects])}\\n\"\n            if questions:\n                dialogue_context += \"Sample questions:\\n\" + \"\\n\".join([f\"  • \\\"{q}\\\"\" for q in questions])\n            cluster_context_formatted.append(dialogue_context)\n    \n    cluster_formatted = \"\\n\\n\".join(cluster_context_formatted)\n    \n    user_prompt = f\"\"\"\n**CURRENT DIALOGUE FOCUS**\nTopic: \"{current_topic}\"\nSpecific Aspects: {current_aspects_formatted}\nAlready Asked Questions:\n{current_questions_formatted}\n\n**ENRICHMENT FROM TOPIC CLUSTER CONTEXT**\n{cluster_formatted}\n\nGenerate 8 NEW questions that would naturally follow in the CURRENT dialogue, leveraging insights from the cluster context while staying focused on the current dialogue's specific aspects:\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n\n    response = pipe(\n        messages,\n        temperature=temperature,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n    \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n    \n    # Extract JSON from response\n    try:\n        result = json.loads(generated_text)\n        if \"questions\" in result and isinstance(result[\"questions\"], list):\n            # Filter out empty questions and limit to 20\n            questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n            return questions\n    except json.JSONDecodeError:\n        # Try to find JSON pattern in the response\n        json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n        if json_match:\n            result = json.loads(json_match.group(0))\n            if \"questions\" in result and isinstance(result[\"questions\"], list):\n                questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n                return questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T05:01:02.945349Z","iopub.execute_input":"2026-01-18T05:01:02.945991Z","iopub.status.idle":"2026-01-18T05:01:02.957292Z","shell.execute_reply.started":"2026-01-18T05:01:02.945960Z","shell.execute_reply":"2026-01-18T05:01:02.956576Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"results_app2={}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:35:05.249869Z","iopub.execute_input":"2026-01-05T13:35:05.250415Z","iopub.status.idle":"2026-01-05T13:35:05.253857Z","shell.execute_reply.started":"2026-01-05T13:35:05.250384Z","shell.execute_reply":"2026-01-05T13:35:05.253186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for cl in range(len(clusters)):\n#     print(cl)\n#     cluster_questions = extract_cluster_questions(df, clusters[cl])\n\n#     cluster_aspects = extract_cluster_aspects(pipe, clusters[cl], cluster_questions)\n    \n#     gen_questions = generate_cluster_questions(pipe, clusters[cl], cluster_aspects, cluster_questions)\n\n#     comparisons = {}\n#     remaining_questions = extract_cluster_last_questions(df, clusters[cl])\n#     for i, q_a in enumerate(gen_questions):\n#         comparisons[q_a] = {}\n#         for j, q_b in enumerate(remaining_questions):\n#             confidence = compare_questions(pipe, q_a, q_b)\n#             comparisons[q_a][q_b] = confidence\n\n#     results_app2[f'cluster {cl}'] = {\n#     \"topic\": clusters[cl],\n#     \"aspects\": cluster_aspects,\n#     \"gen_questions\": gen_questions,\n#     \"comparisons\": comparisons\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for cl in range(len(clusters)):\n#     print(cl)\n#     cluster_questions = extract_cluster_questions(df, clusters[cl])\n\n#     cluster_aspects = extract_cluster_aspects(pipe, clusters[cl], cluster_questions)\n    \n#     gen_questions = generate_cluster_questions(pipe, clusters[cl], cluster_aspects, cluster_questions)\n\n#     comparisons = {}\n#     remaining_questions = extract_cluster_last_questions(df, clusters[cl])\n#     for i, q_a in enumerate(gen_questions):\n#         comparisons[q_a] = {}\n#         for j, q_b in enumerate(remaining_questions):\n#             confidence = compare_questions(pipe, q_a, q_b)\n#             comparisons[q_a][q_b] = confidence\n\n#     results_app2[f'cluster {cl}'] = {\n#     \"topic\": clusters[cl],\n#     \"aspects\": cluster_aspects,\n#     \"gen_questions\": gen_questions,\n#     \"comparisons\": comparisons\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for cl_idx, cluster_dialogues in enumerate(clusters.values()):\n    print(f\"Processing cluster {cl_idx} with {len(cluster_dialogues)} dialogues\")\n    \n    # Get first 60% questions for all dialogues in this cluster\n    cluster_questions_dict = extract_cluster_questions(df, cluster_dialogues)\n    \n    # Pre-extract aspects for each dialogue using cluster context\n    dialogue_aspects = {}\n    for dialogue_idx, topic in cluster_dialogues:\n        # Get current dialogue questions\n        current_questions = cluster_questions_dict[dialogue_idx]['first_60_percent']\n        \n        # Prepare cluster context: questions from OTHER dialogues in cluster\n        cluster_context_questions = [\n            cluster_questions_dict[other_idx]['first_60_percent'] \n            for other_idx in cluster_questions_dict.keys() \n            if other_idx != dialogue_idx\n        ]\n        \n        # Extract aspects using enriched cluster context\n        dialogue_aspects[dialogue_idx] = extract_aspects_cluster(\n            pipe,\n            current_topic=topic,\n            current_questions=current_questions,\n            cluster_questions=cluster_context_questions\n        )\n    \n    # Generate questions and perform comparisons for each dialogue\n    cluster_results = {\"dialogues\": {}}\n\n    for dialogue_idx, topic in cluster_dialogues:\n            \n            # Get current dialogue data\n            current_questions = cluster_questions_dict[dialogue_idx]['first_60_percent']\n            current_aspects = dialogue_aspects[dialogue_idx]\n            \n            # Prepare cluster context for generation: aspects + questions from OTHER dialogues\n            cluster_context = []\n            for other_idx, other_topic in cluster_dialogues:\n                if other_idx != dialogue_idx:\n                    cluster_context.append({\n                        'aspects': dialogue_aspects[other_idx],\n                        'questions': cluster_questions_dict[other_idx]['first_60_percent']\n                    })\n            \n            # Generate questions using enriched context\n            gen_questions = generate_questions_cluster(\n                pipe,\n                current_topic=topic,\n                current_aspects=current_aspects,\n                current_questions=current_questions,\n                cluster_context=cluster_context\n            )\n            \n            # Get remaining questions (last 40%) for THIS dialogue only\n            remaining_questions = extract_cluster_last_questions(df, [(dialogue_idx, topic)])\n            dialogue_remaining = remaining_questions[dialogue_idx]['last_40_percent']\n            \n            # Compare generated questions with remaining questions\n            comparisons = {}\n            for q_a in gen_questions:\n                comparisons[q_a] = {}\n                for q_b in dialogue_remaining:\n                    confidence = compare_questions(pipe, q_a, q_b)\n                    comparisons[q_a][q_b] = confidence\n            \n            # Store results for this dialogue\n            cluster_results[\"dialogues\"][dialogue_idx] = {\n                \"topic\": topic,\n                \"aspects\": current_aspects,\n                \"gen_questions\": gen_questions,\n                \"comparisons\": comparisons\n            }\n        \n    # Store cluster results\n    results_app2[f'cluster {cl_idx}'] = cluster_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clusters_number = [11, 12, 14, 15, 16, 18, 19]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for cl in clusters_number:\n#     print(cl)\n#     cluster_questions = extract_cluster_questions(df, clusters[cl])\n#     cluster_aspects = extract_cluster_aspects(pipe, clusters[cl], cluster_questions)\n#     gen_questions = generate_cluster_questions(pipe, clusters[cl], cluster_aspects, cluster_questions)\n    \n#     # Create all question pairs for comparison\n#     remaining_questions = extract_cluster_last_questions(df, clusters[cl])\n#     question_pairs = [(q_a, q_b) for q_a in gen_questions for q_b in remaining_questions]\n    \n#     # Batch comparison - much faster!\n#     batch_results = batch_compare_questions(pipe, question_pairs, batch_size=10)\n    \n#     # Reconstruct comparisons dictionary from batch results\n#     comparisons = {q_a: {} for q_a in gen_questions}\n#     for (q_a, q_b), confidence in batch_results.items():\n#         comparisons[q_a][q_b] = confidence\n    \n#     results_app2[f'cluster {cl}'] = {\n#         \"topic\": clusters[cl],\n#         \"aspects\": cluster_aspects,\n#         \"gen_questions\": gen_questions,\n#         \"comparisons\": comparisons\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check1 = ['What kind of materials are stored in the secret archives beyond printed books?',\n#           'Are there any restrictions on who can access the secret archives?',\n#           'What historical events influenced the decision to move the secret archives?',\n#           'How has the purpose of the Vat evolved since its opening?',\n#           'Are there any known classified topics or forbidden subjects in the secret collection?',\n#           'Has the size of the secret archives grown over time?']\n\n# check2 = ['What is the Vat the library of?',\n#           'How many books survived the Pre Lateran period?',\n#           'how many printed books does it contain?',\n#           'when were the Secret Archives moved from the rest of the library?',\n#           'what is the point of the project started in 2014?',\n#           'what will this allow?']\n# row=1\n# question_pairs = [(q_a, q_b) for q_a in check1 for q_b in check2]\n\n# batch_results = batch_compare_questions(pipe, question_pairs, batch_size=10)\n\n# comparisons = {q_a: {} for q_a in check1}\n# for (q_a, q_b), confidence in batch_results.items():\n#     comparisons[q_a][q_b] = confidence\n\n# results_app2[f'row {row}'] = {\n#     \"topic\": clusters[0],\n#     \"gen_questions\": check1,\n#     \"comparisons\": comparisons\n# }\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:40:25.487146Z","iopub.execute_input":"2026-01-05T13:40:25.487733Z","iopub.status.idle":"2026-01-05T13:42:11.069705Z","shell.execute_reply.started":"2026-01-05T13:40:25.487683Z","shell.execute_reply":"2026-01-05T13:42:11.069092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_app2_df = pd.DataFrame(results_app2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:42:11.070858Z","iopub.execute_input":"2026-01-05T13:42:11.071223Z","iopub.status.idle":"2026-01-05T13:42:11.091982Z","shell.execute_reply.started":"2026-01-05T13:42:11.071200Z","shell.execute_reply":"2026-01-05T13:42:11.091377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_app2_df.T.to_csv('/kaggle/working/results_app2_df.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_df.T['comparisons'].iloc[4]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# next(iter(next(iter(results_df.T['comparisons'].iloc[0].values())).items()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# messages = [\n#     {\"role\": \"system\", \"content\": sys_prompt.strip()},\n#     {\"role\": \"user\", \"content\": prompt.strip()}\n# ]\n# pipe(messages)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}