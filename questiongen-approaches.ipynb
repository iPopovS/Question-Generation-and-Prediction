{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14398320,"sourceType":"datasetVersion","datasetId":9195505},{"sourceId":14533393,"sourceType":"datasetVersion","datasetId":9282286}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom typing import List, Dict, Tuple, Optional\nimport json\nimport re\nimport torch\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:02.382234Z","iopub.execute_input":"2026-01-18T04:43:02.382490Z","iopub.status.idle":"2026-01-18T04:43:07.688031Z","shell.execute_reply.started":"2026-01-18T04:43:02.382459Z","shell.execute_reply":"2026-01-18T04:43:07.687244Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet'}\ndf = pd.read_parquet(\"hf://datasets/stanfordnlp/coqa/\" + splits[\"train\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:07.689595Z","iopub.execute_input":"2026-01-18T04:43:07.690023Z","iopub.status.idle":"2026-01-18T04:43:12.519395Z","shell.execute_reply.started":"2026-01-18T04:43:07.689988Z","shell.execute_reply":"2026-01-18T04:43:12.518834Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Getting dialogues with 20 questions from the dataset","metadata":{}},{"cell_type":"code","source":"rows_numb = []\nfor row in range(len(df['questions'])):\n    if len(df['questions'][row]) == 20:\n        rows_numb.append(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:12.520305Z","iopub.execute_input":"2026-01-18T04:43:12.520541Z","iopub.status.idle":"2026-01-18T04:43:12.553109Z","shell.execute_reply.started":"2026-01-18T04:43:12.520519Z","shell.execute_reply":"2026-01-18T04:43:12.552441Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"rows_100 = rows_numb[:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows_1000 = rows_numb[:1000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:12.553758Z","iopub.execute_input":"2026-01-18T04:43:12.553983Z","iopub.status.idle":"2026-01-18T04:43:12.570715Z","shell.execute_reply.started":"2026-01-18T04:43:12.553954Z","shell.execute_reply":"2026-01-18T04:43:12.570084Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def extract_json_from_response(response: str) -> Optional[Dict]:\n    \"\"\"Extract and parse JSON from LLM response with error handling.\"\"\"\n    try:\n        # Try direct JSON parsing first\n        return json.loads(response)\n    except json.JSONDecodeError:\n        # Look for JSON-like structure in the response\n        json_match = re.search(r'\\{[^{}]*\\}', response)\n        if json_match:\n            try:\n                return json.loads(json_match.group(0))\n            except json.JSONDecodeError:\n                pass\n    \n    # Fallback: try to find confidence value directly\n    confidence_match = re.search(r'\"confidence\":\\s*([0-9]*\\.?[0-9]+)', response)\n    if confidence_match:\n        return {\"confidence\": float(confidence_match.group(1))}\n    \n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:43:12.571518Z","iopub.execute_input":"2026-01-18T04:43:12.571791Z","iopub.status.idle":"2026-01-18T04:43:12.585532Z","shell.execute_reply.started":"2026-01-18T04:43:12.571767Z","shell.execute_reply":"2026-01-18T04:43:12.584888Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-4B-Instruct-2507\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Approach 1","metadata":{}},{"cell_type":"markdown","source":"### Step 1: extract topic\n\nIn this approach, topics are extracted based solely on the first 60% of users’ questions. These questions will be refered as promt questions (PQ) going forward.","metadata":{}},{"cell_type":"code","source":"def extract_main_topic(pipe, questions: list[str], temperature: float = 0.7) -> str:\n    \"\"\"Extract the main topic/theme from a set of related questions from the same user context.\"\"\"\n    \n    system_prompt = \"\"\"\nYou are a topic extraction expert. Your task is to analyze multiple questions from the same user and identify the single main topic or theme that connects all of them.\n\n**Instructions:**\n1. Analyze all the provided questions together\n2. Identify the core subject, domain, or theme that unifies these questions\n3. The questions are from the same user context, so they likely revolve around one central topic\n4. Output ONLY a JSON object with a single \"topic\" field\n5. The main topic should be concise (2-8 words maximum) and capture the essence of all questions\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"topic\": \"possibility of intelligent life existing on other planets\"\n}\n\"\"\"\n    \n    questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n    user_prompt = f\"\"\"\n**Questions from the same user context:**\n{questions_formatted}\n\nNow analyze these questions and extract the single main topic that connects them all:\n\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n\n    response = pipe(\n            messages,\n            temperature=temperature,\n            do_sample=False,\n            pad_token_id=pipe.tokenizer.eos_token_id\n        )\n        \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n    \n    result = json.loads(generated_text)\n    if \"topic\" in result:\n        return str(result[\"topic\"]).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T04:32:49.297851Z","iopub.execute_input":"2026-01-04T04:32:49.298484Z","iopub.status.idle":"2026-01-04T04:32:49.305306Z","shell.execute_reply.started":"2026-01-04T04:32:49.298456Z","shell.execute_reply":"2026-01-04T04:32:49.304736Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 2: extract aspects\n\nNext, aspects are identified using both topic and PQ.","metadata":{}},{"cell_type":"code","source":"def extract_topic_aspects(pipe, topic: str, questions: list[str], temperature: float = 0.7) -> list[str]:\n    \"\"\"Extract specific aspects/details of a known topic that the user is interested in based on their questions.\"\"\"\n    \n    system_prompt = \"\"\"\nYou are an aspect extraction expert. Your task is to analyze questions about a specific topic and identify the key aspects, details, or dimensions of that topic the user is interested in.\n\n**Instructions:**\n1. You are given a main topic and multiple questions about that topic from the same user\n2. Analyze the questions to identify what specific aspects, details, or sub-topics of the main topic the user cares about\n3. Focus on concrete details mentioned or implied in the questions\n4. Output ONLY a JSON object with a single \"aspects\" field containing a list of strings\n5. Each aspect should be concise (2-6 words) and represent a distinct detail or dimension of the main topic\n6. Include only aspects that are clearly supported by the questions\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"aspects\": [\"scientific reasoning for existence\", \"detection methods and technology\", \"expert predictions and timeline\"]\n}\n\n**Examples:**\n- Topic: \"possibility of intelligent life existing on other planets\" + Questions about evidence, which planet, methods, timeline, and experts → [age of universe, Earth-like planets,  radio telescopes, timeline for contact]\n- Topic: \"university\" + Questions about founders, buildings, programs → [\"founding history\", \"campus architecture\", \"academic programs\"]\n\"\"\"\n    \n    questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n    user_prompt = f\"\"\"\n**Main Topic:** \"{topic}\"\n\n**User Questions about this topic:**\n{questions_formatted}\n\nNow analyze these questions and extract the specific aspects or details of \"{topic}\" that the user is interested in:\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n    \n    response = pipe(\n        messages,\n        temperature=temperature,\n        do_sample=False,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n    \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n\n    result = json.loads(generated_text)\n    if \"aspects\" in result and isinstance(result[\"aspects\"], list):\n        return [str(aspect).strip() for aspect in result[\"aspects\"] if aspect]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T11:39:04.510215Z","iopub.execute_input":"2026-01-03T11:39:04.510516Z","iopub.status.idle":"2026-01-03T11:39:04.521842Z","shell.execute_reply.started":"2026-01-03T11:39:04.510487Z","shell.execute_reply":"2026-01-03T11:39:04.520815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3: generate questions\nGenerate 8 questions using obtained topic, aspects and PQ.","metadata":{}},{"cell_type":"code","source":"def generate_questions(pipe, topic: str, aspects: list[str], questions: list[str], temperature: float = 0.7) -> list[str]:\n    \"\"\"Generate possible follow-up questions a user might ask based on topic, aspects, and previously asked questions.\"\"\"\n    \n    system_prompt = \"\"\"\nYou are a question generation expert. Your task is to generate new, relevant questions that a user might ask about a specific topic, based on the aspects they're interested in and avoiding questions they've already asked.\n\n**Instructions:**\n1. You are given a main topic, key aspects of that topic, and questions the user has already asked\n2. Generate 8 NEW questions that:\n   - Are relevant to the main topic\n   - Cover the specified aspects that haven't been fully explored yet\n   - Are different from the already asked questions (avoid rephrasing the same questions)\n   - Sound natural and like something a real user would ask\n   - Focus on details, specifics, or related information the user might want to know next\n3. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n4. Each question should be clear, concise, and self-contained\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"questions\": [\n    \"What galaxy do we live in?\",\n    \"How many stars does it have?\",\n    \"Does Shostak believe beings from space could make contact with Earth soon?\"\n  ]\n}\n\n**Important Guidelines:**\n- DO NOT repeat or rephrase the already asked questions\n- DO cover different aspects or dive deeper into aspects that weren't fully explored\n- DO make questions specific and actionable\n- DO maintain a natural, conversational tone\n- DO focus on what the user might want to know next based on their interests\n\"\"\"\n    \n    aspects_formatted = \", \".join([f'\"{aspect}\"' for aspect in aspects])\n    questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n    user_prompt = f\"\"\"\n**Main Topic:** \"{topic}\"\n\n**Key Aspects of Interest:** {aspects_formatted}\n\n**Questions Already Asked:**\n{questions_formatted}\n\nNow generate new, relevant questions that the user might ask next. These questions should explore the topic and aspects in new ways, avoiding repetition of the already asked questions:\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n\n    response = pipe(\n        messages,\n        temperature=temperature,\n        do_sample=True,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n    \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n\n    try:\n        result = json.loads(generated_text)\n        if \"questions\" in result and isinstance(result[\"questions\"], list):\n            # Filter out empty questions and limit to 20\n            questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n            return questions\n    except json.JSONDecodeError:\n        # Try to find JSON pattern in the response\n        json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n        if json_match:\n            result = json.loads(json_match.group(0))\n            if \"questions\" in result and isinstance(result[\"questions\"], list):\n                questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n                return questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T20:58:38.508116Z","iopub.execute_input":"2026-01-20T20:58:38.508473Z","iopub.status.idle":"2026-01-20T20:58:38.523006Z","shell.execute_reply.started":"2026-01-20T20:58:38.508445Z","shell.execute_reply":"2026-01-20T20:58:38.522249Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Step 4: use LLM-as-a-judge technique to compare questions\nThis function is comparing one question with another on a scale from 0 to 1, where 0 means that questions are absolutely different and 1 means that they are definitely equivalent.","metadata":{}},{"cell_type":"code","source":"def compare_questions(pipe, question1: str, question2: str, \n                     temperature: float = 0.7, max_new_tokens: int = 100) -> float:\n\n    sys_prompt = \"\"\"\nYou are a semantic equivalence evaluator. Your task is to compare two questions and determine the confidence that they are semantically equivalent.\n\n**Instructions:**\n1. Compare the meaning and intent of the two questions\n2. Determine if they're asking for the same information and would yield the same answer\n3. Output ONLY a JSON object with a single \"confidence\" field\n4. The confidence must be a number between 0 and 1\n\n**Explanation of confidence scores:**\n- 0.9-1.0: Definitely equivalent (same meaning, intent, and expected answer)\n- 0.7-0.9: Highly likely equivalent (minor wording differences only)\n- 0.5-0.7: Possibly equivalent (similar intent but different phrasing)\n- 0.3-0.5: Unlikely equivalent (related but different aspects)\n- 0.0-0.3: Definitely different (completely different questions)\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"confidence\": 0.85\n}\n\"\"\"\n\n    prompt = f\"\"\"\n**Questions to compare:**\nQuestion 1: \"{question1}\"\nQuestion 2: \"{question2}\"\n\"\"\"\n\n    messages = [\n    {\"role\": \"system\", \"content\": sys_prompt.strip()},\n    {\"role\": \"user\", \"content\": prompt.strip()}\n]\n\n    response = pipe(\n        messages,\n        temperature=temperature,\n        do_sample=False\n    )\n\n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n\n    result = extract_json_from_response(generated_text)\n    \n    confidence = float(result[\"confidence\"])\n    return max(0.0, min(1.0, confidence))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:36.893784Z","iopub.execute_input":"2026-01-18T04:44:36.894118Z","iopub.status.idle":"2026-01-18T04:44:36.901797Z","shell.execute_reply.started":"2026-01-18T04:44:36.894089Z","shell.execute_reply":"2026-01-18T04:44:36.900342Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Execute first approach and save results. In this work, the first 100 dialogues with 20 questions were considered for this approach.","metadata":{}},{"cell_type":"code","source":"results = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T06:04:52.800764Z","iopub.execute_input":"2026-01-03T06:04:52.801317Z","iopub.status.idle":"2026-01-03T06:04:52.815712Z","shell.execute_reply.started":"2026-01-03T06:04:52.801294Z","shell.execute_reply":"2026-01-03T06:04:52.815098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for row in rows_100:\n    print(row)\n    questions = df['questions'].loc[row].tolist()[:int(len(df['questions'].loc[row].tolist()) * 0.6)]\n    topic = extract_main_topic(pipe, questions)\n    aspects = extract_topic_aspects(pipe, topic, questions)\n    gen_questions = generate_questions(pipe, topic, aspects, questions)\n\n    comparisons = {}\n    remaining_questions = df['questions'].loc[row].tolist()[int(len(df['questions'].loc[row].tolist()) * 0.6):]\n    for i, q_a in enumerate(gen_questions):\n        comparisons[q_a] = {}\n        for j, q_b in enumerate(remaining_questions):\n            confidence = compare_questions(pipe, q_a, q_b)\n            comparisons[q_a][q_b] = confidence\n\n    results_app1[f'row {row}'] = {\n    \"topic\": topic,\n    \"aspects\": aspects,\n    \"gen_questions\": gen_questions,\n    \"comparisons\": comparisons\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_app1_df = pd.DataFrame(results_app1)\nresults_app1_df.T","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app1_df.T.to_csv('/kaggle/working/results_app1.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Approach 2","metadata":{}},{"cell_type":"markdown","source":"### Step 1: collect topics\nThis functions extracts topics from dialogues's context. This step is for further combining dialogues into clusters based on topics. The repository already contains a file with all the extracted topics, so this step is optional.","metadata":{}},{"cell_type":"code","source":"# def extract_text_main_topic(pipe, text: str, temperature: float = 0.7) -> str:\n#     \"\"\"Extract the single main topic/theme from a provided text passage.\"\"\"\n    \n#     system_prompt = \"\"\"\n# You are a topic extraction expert. Your task is to analyze a text passage and identify the single main topic or central theme that best represents the entire content.\n\n# **Instructions:**\n# 1. Carefully read and understand the entire text passage\n# 2. Identify the core subject, central theme, or primary focus of the text\n# 3. The main topic should be the overarching concept that ties all parts of the text together\n# 4. Output ONLY a JSON object with a single \"main_topic\" field\n# 5. The main topic should be concise (2-5 words maximum) and capture the essence of the text\n# 6. Be specific and accurate - avoid generic terms like \"story\" or \"text\"\n\n# **Output format (STRICTLY follow this - no other text):**\n# {\n#   \"main_topic\": \"possibility of intelligent life existing on other planets\"\n# }\n# \"\"\"\n    \n#     user_prompt = f\"\"\"\n# **Text passage to analyze:**\n# {text}\n\n# Now identify the single main topic that best represents this entire text:\n# \"\"\"\n    \n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt.strip()},\n#         {\"role\": \"user\", \"content\": user_prompt.strip()}\n#     ]\n\n#     response = pipe(\n#             messages,\n#             temperature=temperature,\n#             do_sample=False,\n#             pad_token_id=pipe.tokenizer.eos_token_id\n#         )\n        \n#     # Extract generated text\n#     if isinstance(response, list) and len(response) > 0:\n#         if isinstance(response[0], dict) and 'generated_text' in response[0]:\n#             generated_text = response[0]['generated_text'][-1]['content']\n#         else:\n#             generated_text = str(response[0])\n#     else:\n#         generated_text = str(response)\n    \n#     result = json.loads(generated_text)\n#     if \"main_topic\" in result:\n#         return str(result[\"main_topic\"]).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:49:44.066410Z","iopub.execute_input":"2026-01-04T16:49:44.067015Z","iopub.status.idle":"2026-01-04T16:49:44.073614Z","shell.execute_reply.started":"2026-01-04T16:49:44.066985Z","shell.execute_reply":"2026-01-04T16:49:44.072967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_topics = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:50:02.952521Z","iopub.execute_input":"2026-01-04T16:50:02.952834Z","iopub.status.idle":"2026-01-04T16:50:02.956340Z","shell.execute_reply.started":"2026-01-04T16:50:02.952806Z","shell.execute_reply":"2026-01-04T16:50:02.955742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for row in rows_1000:\n#     if row % 10 == 0:\n#         print(row)\n#     main_topic = extract_text_main_topic(pipe, df['story'][row])\n\n#     results_topics[f'row {row}'] = {\n#     \"main_topic\": main_topic\n#     }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:50:03.148838Z","iopub.execute_input":"2026-01-04T16:50:03.149086Z","iopub.status.idle":"2026-01-04T16:50:28.809976Z","shell.execute_reply.started":"2026-01-04T16:50:03.149063Z","shell.execute_reply":"2026-01-04T16:50:28.809378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_topics_df = pd.DataFrame(results_topics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:50:45.603398Z","iopub.execute_input":"2026-01-04T16:50:45.604141Z","iopub.status.idle":"2026-01-04T16:50:45.610276Z","shell.execute_reply.started":"2026-01-04T16:50:45.604109Z","shell.execute_reply":"2026-01-04T16:50:45.609644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_topics_df.T.to_csv('/kaggle/working/topics.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:51:27.846057Z","iopub.execute_input":"2026-01-04T16:51:27.846736Z","iopub.status.idle":"2026-01-04T16:51:27.857867Z","shell.execute_reply.started":"2026-01-04T16:51:27.846704Z","shell.execute_reply":"2026-01-04T16:51:27.857190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"topics = pd.read_csv('/kaggle/input/topics/topics.csv')\nall_topics = topics['main_topic'].tolist()\nrow_identifiers = topics['Unnamed: 0'].str.extract(r'row\\s*(\\d+)')[0].astype(int).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:36.902649Z","iopub.execute_input":"2026-01-18T04:44:36.902977Z","iopub.status.idle":"2026-01-18T04:44:44.792727Z","shell.execute_reply.started":"2026-01-18T04:44:36.902940Z","shell.execute_reply":"2026-01-18T04:44:44.791859Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Step 2: cluster topics using DBSCAN","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import DBSCAN\nimport numpy as np\nfrom scipy.spatial.distance import cosine\n\ndef cluster_topics(topics_list, row_identifiers, min_cluster_size=3, eps=0.4):\n    \"\"\"\n    Cluster topics using sentence embeddings and DBSCAN\n    Returns: dict mapping cluster_id to list of (dialogue_index, topic)\n    \"\"\"\n    # Load pre-trained model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    # Generate embeddings\n    embeddings = model.encode(topics_list, show_progress_bar=True)\n    \n    # Cluster using DBSCAN\n    clustering = DBSCAN(eps=eps, min_samples=min_cluster_size, metric='cosine')\n    cluster_labels = clustering.fit_predict(embeddings)\n \n    clusters = {}\n    for idx, (label, topic) in enumerate(zip(cluster_labels, topics_list)):\n        row_id = row_identifiers[idx] \n        if label == -1:\n            continue\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append((row_id, topic))\n    \n    return clusters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:44.793926Z","iopub.execute_input":"2026-01-18T04:44:44.794248Z","iopub.status.idle":"2026-01-18T04:44:50.249167Z","shell.execute_reply.started":"2026-01-18T04:44:44.794215Z","shell.execute_reply":"2026-01-18T04:44:50.248564Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"clusters = cluster_topics(all_topics, row_identifiers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_cluster_questions(df, cluster_dialogues):\n    cluster_qs = {}\n    \n    for dialogue_idx, topic in cluster_dialogues:\n        # Get all questions for this dialogue\n        all_questions = df['questions'].loc[dialogue_idx].tolist()\n        \n        # Calculate 60% cutoff\n        cutoff_idx = int(len(all_questions) * 0.6)\n        \n        # Extract first 60% of questions\n        first_60_percent = all_questions[:cutoff_idx]\n\n        cluster_qs[dialogue_idx] = {\n            'first_60_percent': first_60_percent\n        }\n    \n    return cluster_qs\n\ndef extract_cluster_last_questions(df, cluster_dialogues):\n    cluster_qs_test = {}\n    \n    for dialogue_idx, topic in cluster_dialogues:\n        # Get all questions for this dialogue\n        all_questions = df['questions'].loc[dialogue_idx].tolist()\n        \n        # Calculate starting index for last 60%\n        start_idx = int(len(all_questions) * 0.6)\n        \n        # Extract last 40% of questions\n        last_40_percent = all_questions[start_idx:]\n\n        cluster_qs_test[dialogue_idx] = {\n            'last_40_percent': last_40_percent\n        }\n    \n    return cluster_qs_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:49:26.549445Z","iopub.execute_input":"2026-01-18T04:49:26.549762Z","iopub.status.idle":"2026-01-18T04:49:26.556663Z","shell.execute_reply.started":"2026-01-18T04:49:26.549735Z","shell.execute_reply":"2026-01-18T04:49:26.555551Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Step 3: extract dialogue's aspects.\nIn addition to the topic and PQ of the dialogue under consideration, PQ from other dialogues of the cluster are also used.","metadata":{}},{"cell_type":"code","source":"def extract_aspects_cluster(pipe, current_topic: str, current_questions: list[str], cluster_questions: list[list[str]], temperature: float = 0.7) -> list[str]:\n    \"\"\"Extract detailed aspects of a topic using context from all dialogues in the same cluster.\"\"\"\n    \n    system_prompt = \"\"\"\nYou are an aspect extraction expert specializing in contextual analysis. Your task is to identify specific details and dimensions of a topic by analyzing questions from:\n1. The current dialogue (primary focus)\n2. Related dialogues in the same topic cluster (providing broader context)\n\n**Instructions:**\n1. Analyze ALL provided questions together - they all relate to the same core topic cluster\n2. Identify specific aspects, details, or dimensions of the main topic that users are interested in\n3. Focus on concrete details mentioned or implied across the dialogues\n4. Prioritize aspects that appear in multiple dialogues OR are deeply explored in the current dialogue\n5. Output ONLY a JSON object with a single \"aspects\" field containing a list of strings\n6. Each aspect should be concise (2-6 words) and represent a distinct detail or sub-topic\n7. Include only aspects strongly supported by the questions\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"aspects\": [\"album release dates\", \"auction bidding processes\", \"memorabilia authentication\"]\n}\n\n**Critical Guidelines:**\n- LEVERAGE CLUSTER CONTEXT: Use questions from related dialogues to discover aspects not explicitly mentioned in the current dialogue\n- AVOID GENERIC ASPECTS: Be specific and concrete (e.g., \"vinyl condition grading\" not just \"records\")\n- PRIORITIZE RECURRING THEMES: Aspects appearing across multiple dialogues are more significant\n- MAINTAIN TOPIC FOCUS: All aspects must directly relate to the core topic cluster\n\"\"\"\n\n    current_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(current_questions)])\n    \n    # Format cluster PQ\n    cluster_context = []\n    for i, dialogue_questions in enumerate(cluster_questions[:3]):  # Limit to 3 dialogues\n        dialogue_sample = dialogue_questions\n        cluster_context.append(f\"Related Dialogue {i+1} questions:\\n\" + \"\\n\".join([f\"  • \\\"{q}\\\"\" for q in dialogue_sample]))\n    cluster_formatted = \"\\n\\n\".join(cluster_context)\n    \n    user_prompt = f\"\"\"\n**CORE TOPIC CLUSTER:** \"{current_topic}\"\n\n**CURRENT DIALOGUE QUESTIONS (primary focus):**\n{current_formatted}\n\n**ADDITIONAL CONTEXT FROM RELATED DIALOGUES (same topic cluster):**\n{cluster_formatted}\n\nAnalyze ALL questions above to extract the most significant specific aspects of \"{current_topic}\" that users care about:\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n\n    response = pipe(\n        messages,\n        temperature=temperature,\n        do_sample=False,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n    \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n\n    result = json.loads(generated_text)\n    if \"aspects\" in result and isinstance(result[\"aspects\"], list):\n        return [str(aspect).strip() for aspect in result[\"aspects\"] if aspect]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T04:44:53.531635Z","iopub.execute_input":"2026-01-18T04:44:53.532049Z","iopub.status.idle":"2026-01-18T04:44:53.562300Z","shell.execute_reply.started":"2026-01-18T04:44:53.532019Z","shell.execute_reply":"2026-01-18T04:44:53.561503Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Step 4: generate questions\nIn addition to the topic, aspects and PQ of the dialogue under consideration, aspects and PQ from other dialogues of the cluster are also used.","metadata":{}},{"cell_type":"code","source":"def generate_questions_cluster(pipe, current_topic: str, current_aspects: list[str], current_questions: list[str], cluster_context: list[dict], temperature: float = 0.7, max_new_tokens: int = 500) -> list[str]:\n    \"\"\"Generate context-aware questions for a dialogue using enriched cluster information.\"\"\"\n    \n    system_prompt = \"\"\"\nYou are an expert question generator specializing in contextual dialogue expansion. Your task is to generate new, relevant questions for a user by leveraging:\n1. The current dialogue's specific focus (primary priority)\n2. Broader context from related dialogues in the same topic cluster\n\n**Instructions:**\n1. Analyze ALL provided information about the topic cluster\n2. Generate 8 NEW questions that:\n   - Are highly relevant to the CURRENT dialogue's specific aspects and questions\n   - Incorporate valuable details discovered from OTHER dialogues in the cluster\n   - Avoid repeating or rephrasing questions already asked in the current dialogue\n   - Explore unexplored dimensions of the topic revealed by the cluster context\n   - Sound natural and conversational (like a real user would ask)\n3. Prioritize questions that:\n   - Bridge the current dialogue's focus with complementary aspects from cluster context\n   - Address obvious gaps where cluster context reveals important aspects missing in current dialogue\n   - Maintain specificity to the current dialogue's unique angle within the broader topic\n4. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n5. Each question must be clear, concise, and self-contained\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"questions\": [\n    \"What authentication methods are used for rare Michael Jackson memorabilia?\",\n    \"How do posthumous album releases affect the value of original memorabilia?\",\n    \"What was the highest bid ever recorded for Jackson's signature glove?\"\n  ]\n}\n\n**Critical Guidelines:**\n- CURRENT DIALOGUE IS PRIMARY: Never generate questions irrelevant to the current dialogue's specific focus\n- CLUSTER CONTEXT IS ENRICHMENT: Use other dialogues to discover NEW angles, not to change the core topic\n- AVOID REPETITION: Do not generate questions semantically equivalent to already asked questions\n- BE SPECIFIC: Leverage concrete details from cluster aspects\n- MAINTAIN CONVERSATIONAL FLOW: Questions should feel like natural follow-ups to the current discussion\n\"\"\"\n    \n    # Format current dialogue information\n    current_aspects_formatted = \", \".join([f'\"{a}\"' for a in current_aspects])\n    current_questions_formatted = \"\\n\".join([f\"- \\\"{q}\\\"\" for q in current_questions])\n    \n    # Format cluster context\n    cluster_context_formatted = []\n    for i, context in enumerate(cluster_context[:3]):\n        aspects = context.get('aspects', [])\n        questions = context.get('questions', [])\n        \n        if aspects or questions:\n            dialogue_context = f\"Related Dialogue {i+1}:\\n\"\n            if aspects:\n                dialogue_context += f\"Key aspects: {', '.join([f'\\\"{a}\\\"' for a in aspects])}\\n\"\n            if questions:\n                dialogue_context += \"Sample questions:\\n\" + \"\\n\".join([f\"  • \\\"{q}\\\"\" for q in questions])\n            cluster_context_formatted.append(dialogue_context)\n    \n    cluster_formatted = \"\\n\\n\".join(cluster_context_formatted)\n    \n    user_prompt = f\"\"\"\n**CURRENT DIALOGUE FOCUS**\nTopic: \"{current_topic}\"\nSpecific Aspects: {current_aspects_formatted}\nAlready Asked Questions:\n{current_questions_formatted}\n\n**ENRICHMENT FROM TOPIC CLUSTER CONTEXT**\n{cluster_formatted}\n\nGenerate 8 NEW questions that would naturally follow in the CURRENT dialogue, leveraging insights from the cluster context while staying focused on the current dialogue's specific aspects:\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n\n    response = pipe(\n        messages,\n        temperature=temperature,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n    \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n\n    try:\n        result = json.loads(generated_text)\n        if \"questions\" in result and isinstance(result[\"questions\"], list):\n            questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n            return questions\n    except json.JSONDecodeError:\n        json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n        if json_match:\n            result = json.loads(json_match.group(0))\n            if \"questions\" in result and isinstance(result[\"questions\"], list):\n                questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n                return questions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T05:01:02.945349Z","iopub.execute_input":"2026-01-18T05:01:02.945991Z","iopub.status.idle":"2026-01-18T05:01:02.957292Z","shell.execute_reply.started":"2026-01-18T05:01:02.945960Z","shell.execute_reply":"2026-01-18T05:01:02.956576Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Execute second approach and save results.","metadata":{}},{"cell_type":"code","source":"results_app2={}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:35:05.249869Z","iopub.execute_input":"2026-01-05T13:35:05.250415Z","iopub.status.idle":"2026-01-05T13:35:05.253857Z","shell.execute_reply.started":"2026-01-05T13:35:05.250384Z","shell.execute_reply":"2026-01-05T13:35:05.253186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for cl_idx, cluster_dialogues in enumerate(clusters.values()):\n    print(f\"Processing cluster {cl_idx}\")\n\n    cluster_questions_dict = extract_cluster_questions(df, cluster_dialogues)\n\n    dialogue_aspects = {}\n    for dialogue_idx, topic in cluster_dialogues:\n        current_questions = cluster_questions_dict[dialogue_idx]['first_60_percent']\n        cluster_context_questions = [\n            cluster_questions_dict[other_idx]['first_60_percent'] \n            for other_idx in cluster_questions_dict.keys() \n            if other_idx != dialogue_idx\n        ]\n\n        dialogue_aspects[dialogue_idx] = extract_aspects_cluster(\n            pipe,\n            current_topic=topic,\n            current_questions=current_questions,\n            cluster_questions=cluster_context_questions\n        )\n    \n    # Generate questions and perform comparisons for each dialogue\n    cluster_results = {\"dialogues\": {}}\n\n    for dialogue_idx, topic in cluster_dialogues:\n        current_questions = cluster_questions_dict[dialogue_idx]['first_60_percent']\n        current_aspects = dialogue_aspects[dialogue_idx]\n\n        cluster_context = []\n        for other_idx, other_topic in cluster_dialogues:\n            if other_idx != dialogue_idx:\n                cluster_context.append({\n                    'aspects': dialogue_aspects[other_idx],\n                    'questions': cluster_questions_dict[other_idx]['first_60_percent']\n                })\n\n        gen_questions = generate_questions_cluster(\n            pipe,\n            current_topic=topic,\n            current_aspects=current_aspects,\n            current_questions=current_questions,\n            cluster_context=cluster_context\n        )\n\n        remaining_questions = extract_cluster_last_questions(df, [(dialogue_idx, topic)])\n        dialogue_remaining = remaining_questions[dialogue_idx]['last_40_percent']\n        \n        # Compare generated questions with remaining questions\n        comparisons = {}\n        for q_a in gen_questions:\n            comparisons[q_a] = {}\n            for q_b in dialogue_remaining:\n                confidence = compare_questions(pipe, q_a, q_b)\n                comparisons[q_a][q_b] = confidence\n        \n        # Store results for this dialogue\n        cluster_results[\"dialogues\"][dialogue_idx] = {\n            \"topic\": topic,\n            \"aspects\": current_aspects,\n            \"gen_questions\": gen_questions,\n            \"comparisons\": comparisons\n        }\n\n    results_app2[f'cluster {cl_idx}'] = cluster_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_app2_df = pd.DataFrame(results_app2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T13:42:11.070858Z","iopub.execute_input":"2026-01-05T13:42:11.071223Z","iopub.status.idle":"2026-01-05T13:42:11.091982Z","shell.execute_reply.started":"2026-01-05T13:42:11.071200Z","shell.execute_reply":"2026-01-05T13:42:11.091377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app2_df.T.to_csv('/kaggle/working/results_app2_df.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Approach 3","metadata":{}},{"cell_type":"markdown","source":"### Step 1: generate questions based on dialogue's context and PQ","metadata":{}},{"cell_type":"code","source":"def generate_questions_from_text_with_questions(pipe, text: str, questions: list[str], temperature: float = 0.7, max_new_tokens: int = 500) -> list[str]:\n    \n    system_prompt = \"\"\"\nYou are a question generation expert. Your task is to analyze a given text passage and generate relevant, insightful questions that a reader might ask about the content, while avoiding questions that have already been asked.\n\n**Instructions:**\n1. Carefully read and understand the provided text\n2. Review the list of already asked questions to avoid duplication\n3. Generate 8 diverse questions that:\n   - Cover key facts, details, and concepts from the text that haven't been explored yet\n   - Explore implications, context, and deeper meaning not covered in existing questions\n   - Ask about specific examples, dates, names, or events mentioned in the text\n   - Probe relationships between different elements in the text\n   - Question assumptions or explore alternative perspectives\n   - Are natural and conversational (not robotic)\n4. Ensure questions are:\n   - Clear, concise, and self-contained\n   - Based directly on the text content (don't invent facts)\n   - Varied in type (who, what, when, where, why, how, etc.)\n   - Appropriate for the text's complexity and tone\n   - COMPLETELY DIFFERENT from the already asked questions (no rephrasing or similar intent)\n5. Output ONLY a JSON object with a single \"questions\" field containing a list of strings\n6. Include exactly 8 questions - no more, no less\n\n**Output format (STRICTLY follow this - no other text):**\n{\n  \"questions\": [\n    \"What galaxy do we live in?\",\n    \"How many stars does it have?\",\n    \"Does Shostak believe beings from space could make contact with Earth soon?\"\n  ]\n}\n\"\"\"\n\n    questions_formatted = \"\\n\".join([f\"- Question {i+1}: \\\"{q}\\\"\" for i, q in enumerate(questions)])\n    \n    user_prompt = f\"\"\"\n**Text to analyze:**\n{text}\n\n**Questions Already Asked:**\n{questions_formatted}\n\nNow generate 8 thoughtful questions based on this text content:\n\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt.strip()},\n        {\"role\": \"user\", \"content\": user_prompt.strip()}\n    ]\n    \n    response = pipe(\n        messages,\n        temperature=temperature,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n    \n    # Extract generated text\n    if isinstance(response, list) and len(response) > 0:\n        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n            generated_text = response[0]['generated_text'][-1]['content']\n        else:\n            generated_text = str(response[0])\n    else:\n        generated_text = str(response)\n\n    try:\n        result = json.loads(generated_text)\n        if \"questions\" in result and isinstance(result[\"questions\"], list):\n            # Filter out empty questions and limit to 20\n            questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n            return questions\n    except json.JSONDecodeError:\n        # Try to find JSON pattern in the response\n        json_match = re.search(r'\\{[^{}]*\"questions\"[^{}]*\\}', generated_text)\n        if json_match:\n            result = json.loads(json_match.group(0))\n            if \"questions\" in result and isinstance(result[\"questions\"], list):\n                questions = [str(q).strip() for q in result[\"questions\"] if q and q.strip()]\n                return questions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Execute third approach and save results","metadata":{}},{"cell_type":"code","source":"reults_app3 = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for row in rows_100:\n    print(row)\n    remaining_questions = df['questions'].loc[row].tolist()[int(len(df['questions'].loc[row].tolist()) * 0.6):]\n    gen_questions = generate_questions_from_text_with_questions(pipe, df['story'][row], remaining_questions)\n\n    comparisons = {}\n    for i, q_a in enumerate(gen_questions):\n        comparisons[q_a] = {}\n        for j, q_b in enumerate(remaining_questions):\n            confidence = compare_questions(pipe, q_a, q_b)\n            comparisons[q_a][q_b] = confidence\n\n    results_app3[f'row {row}'] = {\n    \"gen_questions\": gen_questions,\n    \"comparisons\": comparisons\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_app3_df = pd.DataFrame(results_app3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results_app3_df.to_csv('/kaggle/working/results_app3.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}